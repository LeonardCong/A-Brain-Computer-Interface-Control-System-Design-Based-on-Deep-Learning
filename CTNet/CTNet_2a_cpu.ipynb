{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e63737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 22, 1000]             512\n",
      "       BatchNorm2d-2          [-1, 8, 22, 1000]              16\n",
      "            Conv2d-3          [-1, 16, 1, 1000]             352\n",
      "       BatchNorm2d-4          [-1, 16, 1, 1000]              32\n",
      "               ELU-5          [-1, 16, 1, 1000]               0\n",
      "         AvgPool2d-6           [-1, 16, 1, 125]               0\n",
      "           Dropout-7           [-1, 16, 1, 125]               0\n",
      "            Conv2d-8           [-1, 16, 1, 125]           4,096\n",
      "       BatchNorm2d-9           [-1, 16, 1, 125]              32\n",
      "              ELU-10           [-1, 16, 1, 125]               0\n",
      "        AvgPool2d-11            [-1, 16, 1, 15]               0\n",
      "          Dropout-12            [-1, 16, 1, 15]               0\n",
      "        Rearrange-13               [-1, 15, 16]               0\n",
      "PatchEmbeddingCNN-14               [-1, 15, 16]               0\n",
      "          Dropout-15               [-1, 15, 16]               0\n",
      "PositioinalEncoding-16               [-1, 15, 16]               0\n",
      "           Linear-17               [-1, 15, 16]             272\n",
      "           Linear-18               [-1, 15, 16]             272\n",
      "           Linear-19               [-1, 15, 16]             272\n",
      "          Dropout-20            [-1, 2, 15, 15]               0\n",
      "           Linear-21               [-1, 15, 16]             272\n",
      "MultiHeadAttention-22               [-1, 15, 16]               0\n",
      "          Dropout-23               [-1, 15, 16]               0\n",
      "        LayerNorm-24               [-1, 15, 16]              32\n",
      "      ResidualAdd-25               [-1, 15, 16]               0\n",
      "           Linear-26               [-1, 15, 64]           1,088\n",
      "             GELU-27               [-1, 15, 64]               0\n",
      "          Dropout-28               [-1, 15, 64]               0\n",
      "           Linear-29               [-1, 15, 16]           1,040\n",
      "          Dropout-30               [-1, 15, 16]               0\n",
      "        LayerNorm-31               [-1, 15, 16]              32\n",
      "      ResidualAdd-32               [-1, 15, 16]               0\n",
      "           Linear-33               [-1, 15, 16]             272\n",
      "           Linear-34               [-1, 15, 16]             272\n",
      "           Linear-35               [-1, 15, 16]             272\n",
      "          Dropout-36            [-1, 2, 15, 15]               0\n",
      "           Linear-37               [-1, 15, 16]             272\n",
      "MultiHeadAttention-38               [-1, 15, 16]               0\n",
      "          Dropout-39               [-1, 15, 16]               0\n",
      "        LayerNorm-40               [-1, 15, 16]              32\n",
      "      ResidualAdd-41               [-1, 15, 16]               0\n",
      "           Linear-42               [-1, 15, 64]           1,088\n",
      "             GELU-43               [-1, 15, 64]               0\n",
      "          Dropout-44               [-1, 15, 64]               0\n",
      "           Linear-45               [-1, 15, 16]           1,040\n",
      "          Dropout-46               [-1, 15, 16]               0\n",
      "        LayerNorm-47               [-1, 15, 16]              32\n",
      "      ResidualAdd-48               [-1, 15, 16]               0\n",
      "           Linear-49               [-1, 15, 16]             272\n",
      "           Linear-50               [-1, 15, 16]             272\n",
      "           Linear-51               [-1, 15, 16]             272\n",
      "          Dropout-52            [-1, 2, 15, 15]               0\n",
      "           Linear-53               [-1, 15, 16]             272\n",
      "MultiHeadAttention-54               [-1, 15, 16]               0\n",
      "          Dropout-55               [-1, 15, 16]               0\n",
      "        LayerNorm-56               [-1, 15, 16]              32\n",
      "      ResidualAdd-57               [-1, 15, 16]               0\n",
      "           Linear-58               [-1, 15, 64]           1,088\n",
      "             GELU-59               [-1, 15, 64]               0\n",
      "          Dropout-60               [-1, 15, 64]               0\n",
      "           Linear-61               [-1, 15, 16]           1,040\n",
      "          Dropout-62               [-1, 15, 16]               0\n",
      "        LayerNorm-63               [-1, 15, 16]              32\n",
      "      ResidualAdd-64               [-1, 15, 16]               0\n",
      "           Linear-65               [-1, 15, 16]             272\n",
      "           Linear-66               [-1, 15, 16]             272\n",
      "           Linear-67               [-1, 15, 16]             272\n",
      "          Dropout-68            [-1, 2, 15, 15]               0\n",
      "           Linear-69               [-1, 15, 16]             272\n",
      "MultiHeadAttention-70               [-1, 15, 16]               0\n",
      "          Dropout-71               [-1, 15, 16]               0\n",
      "        LayerNorm-72               [-1, 15, 16]              32\n",
      "      ResidualAdd-73               [-1, 15, 16]               0\n",
      "           Linear-74               [-1, 15, 64]           1,088\n",
      "             GELU-75               [-1, 15, 64]               0\n",
      "          Dropout-76               [-1, 15, 64]               0\n",
      "           Linear-77               [-1, 15, 16]           1,040\n",
      "          Dropout-78               [-1, 15, 16]               0\n",
      "        LayerNorm-79               [-1, 15, 16]              32\n",
      "      ResidualAdd-80               [-1, 15, 16]               0\n",
      "           Linear-81               [-1, 15, 16]             272\n",
      "           Linear-82               [-1, 15, 16]             272\n",
      "           Linear-83               [-1, 15, 16]             272\n",
      "          Dropout-84            [-1, 2, 15, 15]               0\n",
      "           Linear-85               [-1, 15, 16]             272\n",
      "MultiHeadAttention-86               [-1, 15, 16]               0\n",
      "          Dropout-87               [-1, 15, 16]               0\n",
      "        LayerNorm-88               [-1, 15, 16]              32\n",
      "      ResidualAdd-89               [-1, 15, 16]               0\n",
      "           Linear-90               [-1, 15, 64]           1,088\n",
      "             GELU-91               [-1, 15, 64]               0\n",
      "          Dropout-92               [-1, 15, 64]               0\n",
      "           Linear-93               [-1, 15, 16]           1,040\n",
      "          Dropout-94               [-1, 15, 16]               0\n",
      "        LayerNorm-95               [-1, 15, 16]              32\n",
      "      ResidualAdd-96               [-1, 15, 16]               0\n",
      "           Linear-97               [-1, 15, 16]             272\n",
      "           Linear-98               [-1, 15, 16]             272\n",
      "           Linear-99               [-1, 15, 16]             272\n",
      "         Dropout-100            [-1, 2, 15, 15]               0\n",
      "          Linear-101               [-1, 15, 16]             272\n",
      "MultiHeadAttention-102               [-1, 15, 16]               0\n",
      "         Dropout-103               [-1, 15, 16]               0\n",
      "       LayerNorm-104               [-1, 15, 16]              32\n",
      "     ResidualAdd-105               [-1, 15, 16]               0\n",
      "          Linear-106               [-1, 15, 64]           1,088\n",
      "            GELU-107               [-1, 15, 64]               0\n",
      "         Dropout-108               [-1, 15, 64]               0\n",
      "          Linear-109               [-1, 15, 16]           1,040\n",
      "         Dropout-110               [-1, 15, 16]               0\n",
      "       LayerNorm-111               [-1, 15, 16]              32\n",
      "     ResidualAdd-112               [-1, 15, 16]               0\n",
      "         Flatten-113                  [-1, 240]               0\n",
      "         Dropout-114                  [-1, 240]               0\n",
      "          Linear-115                    [-1, 4]             964\n",
      "================================================================\n",
      "Total params: 25,684\n",
      "Trainable params: 25,684\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.08\n",
      "Forward/backward pass size (MB): 3.43\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 3.61\n",
      "----------------------------------------------------------------\n",
      "Wed Jul 30 03:55:49 2025\n",
      "seed is 1720\n",
      "Subject 1\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "1_0 train_acc: 0.2472 train_loss: 2.655044\tval_acc: 0.264706 val_loss: 1.6779467\n",
      "1_1 train_acc: 0.2472 train_loss: 2.057142\tval_acc: 0.299020 val_loss: 1.5239444\n",
      "1_2 train_acc: 0.3146 train_loss: 1.954976\tval_acc: 0.308824 val_loss: 1.3907524\n",
      "1_3 train_acc: 0.3034 train_loss: 1.831468\tval_acc: 0.313725 val_loss: 1.3577622\n",
      "1_4 train_acc: 0.3408 train_loss: 1.778017\tval_acc: 0.328431 val_loss: 1.2942464\n",
      "1_6 train_acc: 0.3820 train_loss: 1.485711\tval_acc: 0.367647 val_loss: 1.1980897\n",
      "1_7 train_acc: 0.3670 train_loss: 1.454153\tval_acc: 0.460784 val_loss: 1.1130863\n",
      "1_8 train_acc: 0.4082 train_loss: 1.384800\tval_acc: 0.637255 val_loss: 1.0147643\n",
      "1_9 train_acc: 0.4532 train_loss: 1.234884\tval_acc: 0.676471 val_loss: 0.9366793\n",
      "1_10 train_acc: 0.4644 train_loss: 1.261674\tval_acc: 0.671569 val_loss: 0.9180622\n",
      "1_11 train_acc: 0.5206 train_loss: 1.134851\tval_acc: 0.656863 val_loss: 0.8874629\n",
      "1_12 train_acc: 0.5019 train_loss: 1.134429\tval_acc: 0.720588 val_loss: 0.7716733\n",
      "1_13 train_acc: 0.5468 train_loss: 1.094215\tval_acc: 0.666667 val_loss: 0.7574747\n",
      "1_14 train_acc: 0.6404 train_loss: 0.977044\tval_acc: 0.725490 val_loss: 0.7249362\n",
      "1_15 train_acc: 0.6067 train_loss: 0.951283\tval_acc: 0.686275 val_loss: 0.7010099\n",
      "1_17 train_acc: 0.5506 train_loss: 0.945357\tval_acc: 0.725490 val_loss: 0.6696087\n",
      "1_18 train_acc: 0.5955 train_loss: 0.881868\tval_acc: 0.720588 val_loss: 0.6613751\n",
      "1_19 train_acc: 0.6180 train_loss: 0.919676\tval_acc: 0.764706 val_loss: 0.5988052\n",
      "1_20 train_acc: 0.6629 train_loss: 0.817213\tval_acc: 0.779412 val_loss: 0.5760351\n",
      "1_23 train_acc: 0.6816 train_loss: 0.780501\tval_acc: 0.799020 val_loss: 0.5307794\n",
      "1_26 train_acc: 0.6742 train_loss: 0.745008\tval_acc: 0.803922 val_loss: 0.5109364\n",
      "1_28 train_acc: 0.7116 train_loss: 0.689587\tval_acc: 0.779412 val_loss: 0.5040056\n",
      "1_32 train_acc: 0.7004 train_loss: 0.631582\tval_acc: 0.828431 val_loss: 0.4728163\n",
      "1_36 train_acc: 0.7341 train_loss: 0.625224\tval_acc: 0.813725 val_loss: 0.4615249\n",
      "1_39 train_acc: 0.7528 train_loss: 0.557353\tval_acc: 0.828431 val_loss: 0.4406221\n",
      "1_47 train_acc: 0.8165 train_loss: 0.518401\tval_acc: 0.808824 val_loss: 0.4245842\n",
      "1_49 train_acc: 0.7715 train_loss: 0.537648\tval_acc: 0.848039 val_loss: 0.3746540\n",
      "1_55 train_acc: 0.8277 train_loss: 0.436095\tval_acc: 0.857843 val_loss: 0.3706144\n",
      "1_58 train_acc: 0.7715 train_loss: 0.499351\tval_acc: 0.848039 val_loss: 0.3463314\n",
      "1_68 train_acc: 0.8464 train_loss: 0.442583\tval_acc: 0.857843 val_loss: 0.3178670\n",
      "1_76 train_acc: 0.8015 train_loss: 0.457389\tval_acc: 0.872549 val_loss: 0.3144908\n",
      "1_83 train_acc: 0.8614 train_loss: 0.410834\tval_acc: 0.857843 val_loss: 0.3012737\n",
      "1_84 train_acc: 0.8165 train_loss: 0.438928\tval_acc: 0.867647 val_loss: 0.2906048\n",
      "1_88 train_acc: 0.8464 train_loss: 0.399066\tval_acc: 0.892157 val_loss: 0.2582498\n",
      "1_105 train_acc: 0.8277 train_loss: 0.394791\tval_acc: 0.882353 val_loss: 0.2520185\n",
      "1_117 train_acc: 0.8652 train_loss: 0.357649\tval_acc: 0.892157 val_loss: 0.2336049\n",
      "1_127 train_acc: 0.8464 train_loss: 0.337562\tval_acc: 0.916667 val_loss: 0.2070359\n",
      "1_137 train_acc: 0.8277 train_loss: 0.378954\tval_acc: 0.911765 val_loss: 0.1882988\n",
      "1_158 train_acc: 0.8689 train_loss: 0.338160\tval_acc: 0.931373 val_loss: 0.1863354\n",
      "1_160 train_acc: 0.8727 train_loss: 0.291677\tval_acc: 0.931373 val_loss: 0.1750993\n",
      "1_161 train_acc: 0.8839 train_loss: 0.271404\tval_acc: 0.926471 val_loss: 0.1727085\n",
      "1_171 train_acc: 0.8427 train_loss: 0.344569\tval_acc: 0.916667 val_loss: 0.1556192\n",
      "1_180 train_acc: 0.9101 train_loss: 0.249302\tval_acc: 0.931373 val_loss: 0.1503991\n",
      "1_182 train_acc: 0.8689 train_loss: 0.294707\tval_acc: 0.936275 val_loss: 0.1437519\n",
      "1_197 train_acc: 0.8951 train_loss: 0.278385\tval_acc: 0.936275 val_loss: 0.1418169\n",
      "1_198 train_acc: 0.9401 train_loss: 0.161757\tval_acc: 0.941176 val_loss: 0.1280695\n",
      "1_206 train_acc: 0.8989 train_loss: 0.254424\tval_acc: 0.955882 val_loss: 0.1243275\n",
      "1_208 train_acc: 0.8839 train_loss: 0.242423\tval_acc: 0.950980 val_loss: 0.1107570\n",
      "1_227 train_acc: 0.9101 train_loss: 0.217779\tval_acc: 0.960784 val_loss: 0.1013723\n",
      "1_249 train_acc: 0.9064 train_loss: 0.277015\tval_acc: 0.955882 val_loss: 0.0964303\n",
      "1_261 train_acc: 0.8989 train_loss: 0.255378\tval_acc: 0.965686 val_loss: 0.0943596\n",
      "1_266 train_acc: 0.9176 train_loss: 0.246167\tval_acc: 0.955882 val_loss: 0.0909123\n",
      "1_276 train_acc: 0.9326 train_loss: 0.188479\tval_acc: 0.955882 val_loss: 0.0861168\n",
      "1_302 train_acc: 0.9401 train_loss: 0.170086\tval_acc: 0.960784 val_loss: 0.0847713\n",
      "1_307 train_acc: 0.9139 train_loss: 0.199748\tval_acc: 0.980392 val_loss: 0.0601955\n",
      "1_335 train_acc: 0.9513 train_loss: 0.164804\tval_acc: 0.990196 val_loss: 0.0597767\n",
      "1_379 train_acc: 0.9326 train_loss: 0.167035\tval_acc: 0.975490 val_loss: 0.0595811\n",
      "1_405 train_acc: 0.9438 train_loss: 0.171369\tval_acc: 0.975490 val_loss: 0.0566117\n",
      "1_410 train_acc: 0.9588 train_loss: 0.135005\tval_acc: 0.975490 val_loss: 0.0553657\n",
      "1_455 train_acc: 0.9401 train_loss: 0.164077\tval_acc: 0.985294 val_loss: 0.0497954\n",
      "1_477 train_acc: 0.9476 train_loss: 0.146839\tval_acc: 0.995098 val_loss: 0.0451920\n",
      "1_505 train_acc: 0.9176 train_loss: 0.200910\tval_acc: 0.990196 val_loss: 0.0448602\n",
      "1_506 train_acc: 0.9476 train_loss: 0.129862\tval_acc: 0.990196 val_loss: 0.0351754\n",
      "1_539 train_acc: 0.9363 train_loss: 0.155073\tval_acc: 0.995098 val_loss: 0.0338138\n",
      "1_551 train_acc: 0.9251 train_loss: 0.134386\tval_acc: 0.990196 val_loss: 0.0329915\n",
      "1_606 train_acc: 0.9588 train_loss: 0.117811\tval_acc: 0.995098 val_loss: 0.0293510\n",
      "1_625 train_acc: 0.9401 train_loss: 0.138234\tval_acc: 1.000000 val_loss: 0.0168024\n",
      "1_711 train_acc: 0.9588 train_loss: 0.119300\tval_acc: 1.000000 val_loss: 0.0160778\n",
      "1_812 train_acc: 0.9476 train_loss: 0.122940\tval_acc: 1.000000 val_loss: 0.0160182\n",
      "1_824 train_acc: 0.9288 train_loss: 0.181297\tval_acc: 1.000000 val_loss: 0.0158034\n",
      "1_902 train_acc: 0.9700 train_loss: 0.128520\tval_acc: 1.000000 val_loss: 0.0140364\n",
      "1_903 train_acc: 0.9476 train_loss: 0.135006\tval_acc: 1.000000 val_loss: 0.0113559\n",
      "1_941 train_acc: 0.9551 train_loss: 0.104328\tval_acc: 1.000000 val_loss: 0.0110767\n",
      "1_964 train_acc: 0.9625 train_loss: 0.131164\tval_acc: 1.000000 val_loss: 0.0086212\n",
      "1_977 train_acc: 0.9738 train_loss: 0.086242\tval_acc: 1.000000 val_loss: 0.0084438\n",
      "epoch:  977 \tThe test accuracy is: 0.90625\n",
      " THE BEST ACCURACY IS 0.90625\tkappa is 0.875\n",
      "subject 1 duration: 0:47:34.668251\n",
      "seed is 2010\n",
      "Subject 2\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "2_0 train_acc: 0.2809 train_loss: 2.306000\tval_acc: 0.245098 val_loss: 1.5097693\n",
      "2_1 train_acc: 0.3146 train_loss: 2.176466\tval_acc: 0.303922 val_loss: 1.3904707\n",
      "2_2 train_acc: 0.3071 train_loss: 2.038632\tval_acc: 0.313725 val_loss: 1.3234018\n",
      "2_3 train_acc: 0.3071 train_loss: 1.900808\tval_acc: 0.421569 val_loss: 1.2490399\n",
      "2_4 train_acc: 0.3146 train_loss: 1.850868\tval_acc: 0.401961 val_loss: 1.2275198\n",
      "2_5 train_acc: 0.3184 train_loss: 1.766551\tval_acc: 0.480392 val_loss: 1.1946673\n",
      "2_6 train_acc: 0.3446 train_loss: 1.738307\tval_acc: 0.573529 val_loss: 1.1225336\n",
      "2_7 train_acc: 0.3708 train_loss: 1.577366\tval_acc: 0.553922 val_loss: 1.1149813\n",
      "2_8 train_acc: 0.3783 train_loss: 1.588103\tval_acc: 0.598039 val_loss: 1.0589639\n",
      "2_9 train_acc: 0.3371 train_loss: 1.581524\tval_acc: 0.612745 val_loss: 1.0344419\n",
      "2_10 train_acc: 0.4419 train_loss: 1.363147\tval_acc: 0.598039 val_loss: 1.0311884\n",
      "2_11 train_acc: 0.4045 train_loss: 1.502564\tval_acc: 0.632353 val_loss: 0.9892769\n",
      "2_12 train_acc: 0.4307 train_loss: 1.319907\tval_acc: 0.661765 val_loss: 0.9827223\n",
      "2_14 train_acc: 0.4007 train_loss: 1.344764\tval_acc: 0.671569 val_loss: 0.9559543\n",
      "2_15 train_acc: 0.4794 train_loss: 1.234356\tval_acc: 0.710784 val_loss: 0.9339123\n",
      "2_16 train_acc: 0.4532 train_loss: 1.229255\tval_acc: 0.735294 val_loss: 0.9006718\n",
      "2_17 train_acc: 0.4906 train_loss: 1.155420\tval_acc: 0.686275 val_loss: 0.9002510\n",
      "2_18 train_acc: 0.4944 train_loss: 1.137269\tval_acc: 0.686275 val_loss: 0.8746449\n",
      "2_19 train_acc: 0.5318 train_loss: 1.120094\tval_acc: 0.720588 val_loss: 0.8476502\n",
      "2_21 train_acc: 0.5169 train_loss: 1.192031\tval_acc: 0.720588 val_loss: 0.8132123\n",
      "2_23 train_acc: 0.5506 train_loss: 1.022587\tval_acc: 0.764706 val_loss: 0.7840128\n",
      "2_25 train_acc: 0.5843 train_loss: 1.018203\tval_acc: 0.750000 val_loss: 0.7755111\n",
      "2_27 train_acc: 0.5880 train_loss: 0.992931\tval_acc: 0.750000 val_loss: 0.7431052\n",
      "2_31 train_acc: 0.6592 train_loss: 0.876409\tval_acc: 0.769608 val_loss: 0.7154908\n",
      "2_38 train_acc: 0.6180 train_loss: 0.929575\tval_acc: 0.799020 val_loss: 0.6512249\n",
      "2_42 train_acc: 0.6592 train_loss: 0.828580\tval_acc: 0.799020 val_loss: 0.6424797\n",
      "2_46 train_acc: 0.6180 train_loss: 0.862399\tval_acc: 0.813725 val_loss: 0.6410683\n",
      "2_48 train_acc: 0.6330 train_loss: 0.890522\tval_acc: 0.794118 val_loss: 0.6377336\n",
      "2_49 train_acc: 0.6330 train_loss: 0.935095\tval_acc: 0.799020 val_loss: 0.6370740\n",
      "2_51 train_acc: 0.6517 train_loss: 0.873030\tval_acc: 0.794118 val_loss: 0.6110759\n",
      "2_52 train_acc: 0.6217 train_loss: 0.887677\tval_acc: 0.813725 val_loss: 0.5936887\n",
      "2_56 train_acc: 0.6704 train_loss: 0.834081\tval_acc: 0.833333 val_loss: 0.5799813\n",
      "2_66 train_acc: 0.6966 train_loss: 0.764412\tval_acc: 0.848039 val_loss: 0.5335404\n",
      "2_71 train_acc: 0.7228 train_loss: 0.725564\tval_acc: 0.808824 val_loss: 0.5321352\n",
      "2_77 train_acc: 0.6442 train_loss: 0.812122\tval_acc: 0.833333 val_loss: 0.5254921\n",
      "2_78 train_acc: 0.6592 train_loss: 0.864868\tval_acc: 0.843137 val_loss: 0.5046995\n",
      "2_86 train_acc: 0.7191 train_loss: 0.777502\tval_acc: 0.862745 val_loss: 0.4926366\n",
      "2_87 train_acc: 0.7191 train_loss: 0.730695\tval_acc: 0.862745 val_loss: 0.4741517\n",
      "2_90 train_acc: 0.6891 train_loss: 0.771443\tval_acc: 0.852941 val_loss: 0.4719589\n",
      "2_92 train_acc: 0.6779 train_loss: 0.775189\tval_acc: 0.862745 val_loss: 0.4636362\n",
      "2_93 train_acc: 0.6667 train_loss: 0.792522\tval_acc: 0.882353 val_loss: 0.4509461\n",
      "2_100 train_acc: 0.7154 train_loss: 0.675025\tval_acc: 0.872549 val_loss: 0.4416168\n",
      "2_101 train_acc: 0.6966 train_loss: 0.733585\tval_acc: 0.887255 val_loss: 0.4415428\n",
      "2_104 train_acc: 0.6929 train_loss: 0.743005\tval_acc: 0.887255 val_loss: 0.4394377\n",
      "2_105 train_acc: 0.7341 train_loss: 0.710841\tval_acc: 0.897059 val_loss: 0.4128066\n",
      "2_106 train_acc: 0.7266 train_loss: 0.699103\tval_acc: 0.892157 val_loss: 0.4037220\n",
      "2_114 train_acc: 0.6891 train_loss: 0.756075\tval_acc: 0.897059 val_loss: 0.4005247\n",
      "2_123 train_acc: 0.7603 train_loss: 0.634008\tval_acc: 0.916667 val_loss: 0.3852347\n",
      "2_127 train_acc: 0.7303 train_loss: 0.628417\tval_acc: 0.901961 val_loss: 0.3730388\n",
      "2_134 train_acc: 0.7228 train_loss: 0.625850\tval_acc: 0.872549 val_loss: 0.3710572\n",
      "2_136 train_acc: 0.7266 train_loss: 0.647436\tval_acc: 0.901961 val_loss: 0.3583061\n",
      "2_141 train_acc: 0.7079 train_loss: 0.680807\tval_acc: 0.921569 val_loss: 0.3510327\n",
      "2_148 train_acc: 0.7491 train_loss: 0.613431\tval_acc: 0.916667 val_loss: 0.3429803\n",
      "2_150 train_acc: 0.7079 train_loss: 0.642752\tval_acc: 0.921569 val_loss: 0.3408896\n",
      "2_151 train_acc: 0.7790 train_loss: 0.608463\tval_acc: 0.926471 val_loss: 0.3362809\n",
      "2_152 train_acc: 0.7903 train_loss: 0.541210\tval_acc: 0.921569 val_loss: 0.3160653\n",
      "2_160 train_acc: 0.7753 train_loss: 0.617230\tval_acc: 0.916667 val_loss: 0.3122360\n",
      "2_165 train_acc: 0.7491 train_loss: 0.605884\tval_acc: 0.916667 val_loss: 0.3075513\n",
      "2_166 train_acc: 0.7640 train_loss: 0.601490\tval_acc: 0.901961 val_loss: 0.3024986\n",
      "2_177 train_acc: 0.7528 train_loss: 0.617506\tval_acc: 0.926471 val_loss: 0.2980827\n",
      "2_178 train_acc: 0.7303 train_loss: 0.629143\tval_acc: 0.941176 val_loss: 0.2874442\n",
      "2_183 train_acc: 0.7603 train_loss: 0.638592\tval_acc: 0.931373 val_loss: 0.2832903\n",
      "2_184 train_acc: 0.7453 train_loss: 0.604189\tval_acc: 0.936275 val_loss: 0.2770399\n",
      "2_189 train_acc: 0.7603 train_loss: 0.602931\tval_acc: 0.926471 val_loss: 0.2757357\n",
      "2_191 train_acc: 0.7715 train_loss: 0.589952\tval_acc: 0.936275 val_loss: 0.2610790\n",
      "2_195 train_acc: 0.7678 train_loss: 0.606013\tval_acc: 0.950980 val_loss: 0.2609757\n",
      "2_210 train_acc: 0.7154 train_loss: 0.682931\tval_acc: 0.946078 val_loss: 0.2473004\n",
      "2_216 train_acc: 0.7903 train_loss: 0.593370\tval_acc: 0.946078 val_loss: 0.2471218\n",
      "2_221 train_acc: 0.7566 train_loss: 0.628763\tval_acc: 0.946078 val_loss: 0.2324632\n",
      "2_228 train_acc: 0.7491 train_loss: 0.595188\tval_acc: 0.950980 val_loss: 0.2303585\n",
      "2_232 train_acc: 0.7678 train_loss: 0.519898\tval_acc: 0.950980 val_loss: 0.2193203\n",
      "2_235 train_acc: 0.7790 train_loss: 0.611460\tval_acc: 0.946078 val_loss: 0.2179666\n",
      "2_238 train_acc: 0.7715 train_loss: 0.522215\tval_acc: 0.950980 val_loss: 0.2143210\n",
      "2_245 train_acc: 0.7865 train_loss: 0.521694\tval_acc: 0.965686 val_loss: 0.1950790\n",
      "2_255 train_acc: 0.8352 train_loss: 0.479658\tval_acc: 0.970588 val_loss: 0.1946470\n",
      "2_284 train_acc: 0.7753 train_loss: 0.554203\tval_acc: 0.960784 val_loss: 0.1904244\n",
      "2_285 train_acc: 0.8052 train_loss: 0.513449\tval_acc: 0.960784 val_loss: 0.1736812\n",
      "2_286 train_acc: 0.8127 train_loss: 0.534596\tval_acc: 0.965686 val_loss: 0.1735805\n",
      "2_293 train_acc: 0.7978 train_loss: 0.526327\tval_acc: 0.960784 val_loss: 0.1720778\n",
      "2_302 train_acc: 0.8202 train_loss: 0.491294\tval_acc: 0.965686 val_loss: 0.1664229\n",
      "2_304 train_acc: 0.7603 train_loss: 0.551644\tval_acc: 0.980392 val_loss: 0.1641124\n",
      "2_315 train_acc: 0.7865 train_loss: 0.575663\tval_acc: 0.950980 val_loss: 0.1587289\n",
      "2_331 train_acc: 0.8202 train_loss: 0.441244\tval_acc: 0.970588 val_loss: 0.1568393\n",
      "2_332 train_acc: 0.7753 train_loss: 0.552309\tval_acc: 0.980392 val_loss: 0.1566185\n",
      "2_347 train_acc: 0.8127 train_loss: 0.491555\tval_acc: 0.970588 val_loss: 0.1534285\n",
      "2_348 train_acc: 0.8015 train_loss: 0.510668\tval_acc: 0.985294 val_loss: 0.1490040\n",
      "2_362 train_acc: 0.7790 train_loss: 0.593067\tval_acc: 0.985294 val_loss: 0.1338077\n",
      "2_386 train_acc: 0.8240 train_loss: 0.453499\tval_acc: 0.975490 val_loss: 0.1323691\n",
      "2_392 train_acc: 0.8165 train_loss: 0.482871\tval_acc: 0.985294 val_loss: 0.1295048\n",
      "2_400 train_acc: 0.8464 train_loss: 0.490173\tval_acc: 0.975490 val_loss: 0.1217884\n",
      "2_406 train_acc: 0.8127 train_loss: 0.485276\tval_acc: 0.985294 val_loss: 0.1162979\n",
      "2_422 train_acc: 0.8240 train_loss: 0.460258\tval_acc: 0.980392 val_loss: 0.1147688\n",
      "2_423 train_acc: 0.8015 train_loss: 0.495045\tval_acc: 0.980392 val_loss: 0.1040730\n",
      "2_437 train_acc: 0.8240 train_loss: 0.405062\tval_acc: 0.995098 val_loss: 0.1013152\n",
      "2_465 train_acc: 0.8202 train_loss: 0.438321\tval_acc: 0.990196 val_loss: 0.0971960\n",
      "2_469 train_acc: 0.8464 train_loss: 0.421849\tval_acc: 0.995098 val_loss: 0.0884699\n",
      "2_492 train_acc: 0.7903 train_loss: 0.467678\tval_acc: 1.000000 val_loss: 0.0827700\n",
      "2_523 train_acc: 0.8502 train_loss: 0.366690\tval_acc: 0.995098 val_loss: 0.0820383\n",
      "2_535 train_acc: 0.8390 train_loss: 0.413500\tval_acc: 0.990196 val_loss: 0.0779934\n",
      "2_547 train_acc: 0.7940 train_loss: 0.489679\tval_acc: 0.995098 val_loss: 0.0772743\n",
      "2_569 train_acc: 0.8127 train_loss: 0.428813\tval_acc: 1.000000 val_loss: 0.0653956\n",
      "2_583 train_acc: 0.8502 train_loss: 0.400087\tval_acc: 0.995098 val_loss: 0.0618599\n",
      "2_595 train_acc: 0.9026 train_loss: 0.312501\tval_acc: 1.000000 val_loss: 0.0525657\n",
      "2_605 train_acc: 0.8951 train_loss: 0.314855\tval_acc: 0.995098 val_loss: 0.0473831\n",
      "2_612 train_acc: 0.8839 train_loss: 0.319897\tval_acc: 1.000000 val_loss: 0.0469219\n",
      "2_617 train_acc: 0.9064 train_loss: 0.243651\tval_acc: 1.000000 val_loss: 0.0468722\n",
      "2_643 train_acc: 0.8614 train_loss: 0.366130\tval_acc: 0.995098 val_loss: 0.0409487\n",
      "2_670 train_acc: 0.8727 train_loss: 0.338293\tval_acc: 1.000000 val_loss: 0.0373727\n",
      "2_694 train_acc: 0.8727 train_loss: 0.354902\tval_acc: 1.000000 val_loss: 0.0349682\n",
      "2_695 train_acc: 0.9026 train_loss: 0.255168\tval_acc: 1.000000 val_loss: 0.0288776\n",
      "2_731 train_acc: 0.8801 train_loss: 0.294175\tval_acc: 1.000000 val_loss: 0.0260869\n",
      "2_756 train_acc: 0.8727 train_loss: 0.286863\tval_acc: 1.000000 val_loss: 0.0229979\n",
      "2_763 train_acc: 0.8839 train_loss: 0.241872\tval_acc: 1.000000 val_loss: 0.0225655\n",
      "2_802 train_acc: 0.9026 train_loss: 0.276454\tval_acc: 1.000000 val_loss: 0.0206144\n",
      "2_846 train_acc: 0.9213 train_loss: 0.211368\tval_acc: 1.000000 val_loss: 0.0190361\n",
      "2_886 train_acc: 0.9026 train_loss: 0.228257\tval_acc: 1.000000 val_loss: 0.0164305\n",
      "2_909 train_acc: 0.8914 train_loss: 0.238944\tval_acc: 1.000000 val_loss: 0.0148302\n",
      "2_915 train_acc: 0.9288 train_loss: 0.197483\tval_acc: 1.000000 val_loss: 0.0122675\n",
      "2_993 train_acc: 0.8951 train_loss: 0.321862\tval_acc: 1.000000 val_loss: 0.0121267\n",
      "epoch:  993 \tThe test accuracy is: 0.6215277777777778\n",
      " THE BEST ACCURACY IS 0.6215277777777778\tkappa is 0.49537037037037035\n",
      "subject 2 duration: 0:45:43.459586\n",
      "seed is 633\n",
      "Subject 3\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "3_0 train_acc: 0.2509 train_loss: 2.439737\tval_acc: 0.338235 val_loss: 1.4510115\n",
      "3_1 train_acc: 0.2921 train_loss: 2.071293\tval_acc: 0.279412 val_loss: 1.3902906\n",
      "3_2 train_acc: 0.3109 train_loss: 2.022160\tval_acc: 0.313725 val_loss: 1.3085485\n",
      "3_3 train_acc: 0.3258 train_loss: 1.808099\tval_acc: 0.333333 val_loss: 1.2879955\n",
      "3_4 train_acc: 0.3333 train_loss: 1.614395\tval_acc: 0.392157 val_loss: 1.2315176\n",
      "3_5 train_acc: 0.3221 train_loss: 1.685842\tval_acc: 0.450980 val_loss: 1.1564426\n",
      "3_6 train_acc: 0.3558 train_loss: 1.556647\tval_acc: 0.455882 val_loss: 1.1265713\n",
      "3_7 train_acc: 0.4532 train_loss: 1.359452\tval_acc: 0.539216 val_loss: 1.0640352\n",
      "3_8 train_acc: 0.3670 train_loss: 1.448471\tval_acc: 0.637255 val_loss: 0.9902375\n",
      "3_9 train_acc: 0.4345 train_loss: 1.343031\tval_acc: 0.666667 val_loss: 0.9335566\n",
      "3_10 train_acc: 0.4345 train_loss: 1.349613\tval_acc: 0.700980 val_loss: 0.8947569\n",
      "3_11 train_acc: 0.5019 train_loss: 1.209491\tval_acc: 0.750000 val_loss: 0.8637562\n",
      "3_12 train_acc: 0.4757 train_loss: 1.259344\tval_acc: 0.759804 val_loss: 0.7958316\n",
      "3_13 train_acc: 0.5094 train_loss: 1.129846\tval_acc: 0.764706 val_loss: 0.7937813\n",
      "3_14 train_acc: 0.5543 train_loss: 1.084457\tval_acc: 0.750000 val_loss: 0.7575493\n",
      "3_15 train_acc: 0.5243 train_loss: 1.064978\tval_acc: 0.759804 val_loss: 0.6993616\n",
      "3_16 train_acc: 0.5618 train_loss: 1.037693\tval_acc: 0.779412 val_loss: 0.6743395\n",
      "3_18 train_acc: 0.6479 train_loss: 0.941663\tval_acc: 0.774510 val_loss: 0.6419571\n",
      "3_21 train_acc: 0.6517 train_loss: 0.824807\tval_acc: 0.754902 val_loss: 0.6195574\n",
      "3_23 train_acc: 0.6330 train_loss: 0.798834\tval_acc: 0.759804 val_loss: 0.5375921\n",
      "3_24 train_acc: 0.6479 train_loss: 0.834750\tval_acc: 0.789216 val_loss: 0.5341223\n",
      "3_26 train_acc: 0.6742 train_loss: 0.802687\tval_acc: 0.803922 val_loss: 0.4813720\n",
      "3_28 train_acc: 0.7004 train_loss: 0.721359\tval_acc: 0.843137 val_loss: 0.4632143\n",
      "3_30 train_acc: 0.6554 train_loss: 0.810936\tval_acc: 0.862745 val_loss: 0.4306034\n",
      "3_31 train_acc: 0.7191 train_loss: 0.682533\tval_acc: 0.877451 val_loss: 0.4113604\n",
      "3_34 train_acc: 0.7341 train_loss: 0.680124\tval_acc: 0.867647 val_loss: 0.4030941\n",
      "3_35 train_acc: 0.7303 train_loss: 0.648255\tval_acc: 0.877451 val_loss: 0.3531088\n",
      "3_39 train_acc: 0.7828 train_loss: 0.619303\tval_acc: 0.887255 val_loss: 0.3332934\n",
      "3_40 train_acc: 0.7903 train_loss: 0.539340\tval_acc: 0.882353 val_loss: 0.3181497\n",
      "3_41 train_acc: 0.7453 train_loss: 0.650980\tval_acc: 0.882353 val_loss: 0.3096093\n",
      "3_42 train_acc: 0.7678 train_loss: 0.601976\tval_acc: 0.892157 val_loss: 0.3008506\n",
      "3_45 train_acc: 0.8502 train_loss: 0.451581\tval_acc: 0.887255 val_loss: 0.2675492\n",
      "3_51 train_acc: 0.8165 train_loss: 0.458712\tval_acc: 0.901961 val_loss: 0.2335306\n",
      "3_54 train_acc: 0.8390 train_loss: 0.424676\tval_acc: 0.931373 val_loss: 0.2065075\n",
      "3_60 train_acc: 0.8614 train_loss: 0.321246\tval_acc: 0.911765 val_loss: 0.1934977\n",
      "3_64 train_acc: 0.8390 train_loss: 0.413909\tval_acc: 0.936275 val_loss: 0.1682927\n",
      "3_78 train_acc: 0.8577 train_loss: 0.366818\tval_acc: 0.941176 val_loss: 0.1599993\n",
      "3_93 train_acc: 0.8727 train_loss: 0.345794\tval_acc: 0.946078 val_loss: 0.1524792\n",
      "3_99 train_acc: 0.8951 train_loss: 0.263986\tval_acc: 0.970588 val_loss: 0.1238546\n",
      "3_104 train_acc: 0.9101 train_loss: 0.284769\tval_acc: 0.960784 val_loss: 0.1209696\n",
      "3_110 train_acc: 0.8914 train_loss: 0.289126\tval_acc: 0.970588 val_loss: 0.1029529\n",
      "3_115 train_acc: 0.9101 train_loss: 0.253783\tval_acc: 0.955882 val_loss: 0.0972082\n",
      "3_130 train_acc: 0.8876 train_loss: 0.251552\tval_acc: 0.965686 val_loss: 0.0884770\n",
      "3_150 train_acc: 0.8727 train_loss: 0.281847\tval_acc: 0.970588 val_loss: 0.0867746\n",
      "3_160 train_acc: 0.9101 train_loss: 0.267250\tval_acc: 0.975490 val_loss: 0.0721603\n",
      "3_187 train_acc: 0.9288 train_loss: 0.238334\tval_acc: 0.980392 val_loss: 0.0687062\n",
      "3_206 train_acc: 0.9026 train_loss: 0.185674\tval_acc: 0.980392 val_loss: 0.0625980\n",
      "3_238 train_acc: 0.9438 train_loss: 0.171539\tval_acc: 0.985294 val_loss: 0.0480967\n",
      "3_245 train_acc: 0.9251 train_loss: 0.182258\tval_acc: 0.985294 val_loss: 0.0449017\n",
      "3_264 train_acc: 0.9251 train_loss: 0.158871\tval_acc: 0.995098 val_loss: 0.0414571\n",
      "3_269 train_acc: 0.9139 train_loss: 0.226712\tval_acc: 0.990196 val_loss: 0.0337212\n",
      "3_275 train_acc: 0.9438 train_loss: 0.151453\tval_acc: 0.995098 val_loss: 0.0324314\n",
      "3_282 train_acc: 0.9438 train_loss: 0.143930\tval_acc: 0.990196 val_loss: 0.0298348\n",
      "3_376 train_acc: 0.9588 train_loss: 0.117059\tval_acc: 0.995098 val_loss: 0.0294321\n",
      "3_383 train_acc: 0.9139 train_loss: 0.198096\tval_acc: 0.995098 val_loss: 0.0235886\n",
      "3_426 train_acc: 0.9513 train_loss: 0.120198\tval_acc: 0.995098 val_loss: 0.0216076\n",
      "3_441 train_acc: 0.9513 train_loss: 0.148288\tval_acc: 0.995098 val_loss: 0.0213946\n",
      "3_442 train_acc: 0.9476 train_loss: 0.109278\tval_acc: 1.000000 val_loss: 0.0195550\n",
      "3_457 train_acc: 0.9176 train_loss: 0.244865\tval_acc: 0.995098 val_loss: 0.0187937\n",
      "3_461 train_acc: 0.9551 train_loss: 0.123167\tval_acc: 1.000000 val_loss: 0.0173032\n",
      "3_466 train_acc: 0.9476 train_loss: 0.133296\tval_acc: 0.995098 val_loss: 0.0151331\n",
      "3_475 train_acc: 0.9401 train_loss: 0.145913\tval_acc: 0.995098 val_loss: 0.0148090\n",
      "3_478 train_acc: 0.9551 train_loss: 0.133638\tval_acc: 1.000000 val_loss: 0.0129835\n",
      "3_571 train_acc: 0.9700 train_loss: 0.093676\tval_acc: 1.000000 val_loss: 0.0098335\n",
      "3_624 train_acc: 0.9625 train_loss: 0.129929\tval_acc: 1.000000 val_loss: 0.0094194\n",
      "3_626 train_acc: 0.9588 train_loss: 0.116770\tval_acc: 1.000000 val_loss: 0.0077720\n",
      "3_651 train_acc: 0.9925 train_loss: 0.041153\tval_acc: 1.000000 val_loss: 0.0074174\n",
      "3_701 train_acc: 0.9551 train_loss: 0.090503\tval_acc: 1.000000 val_loss: 0.0063769\n",
      "3_785 train_acc: 0.9551 train_loss: 0.095227\tval_acc: 1.000000 val_loss: 0.0048228\n",
      "3_789 train_acc: 0.9513 train_loss: 0.138787\tval_acc: 1.000000 val_loss: 0.0040887\n",
      "3_906 train_acc: 0.9700 train_loss: 0.071081\tval_acc: 1.000000 val_loss: 0.0038001\n",
      "3_950 train_acc: 0.9551 train_loss: 0.111296\tval_acc: 1.000000 val_loss: 0.0037006\n",
      "3_985 train_acc: 0.9513 train_loss: 0.116806\tval_acc: 1.000000 val_loss: 0.0032637\n",
      "epoch:  985 \tThe test accuracy is: 0.9201388888888888\n",
      " THE BEST ACCURACY IS 0.9201388888888888\tkappa is 0.8935185185185185\n",
      "subject 3 duration: 0:47:33.438859\n",
      "seed is 949\n",
      "Subject 4\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "4_0 train_acc: 0.2434 train_loss: 2.674172\tval_acc: 0.245098 val_loss: 1.8466097\n",
      "4_1 train_acc: 0.2846 train_loss: 2.345949\tval_acc: 0.259804 val_loss: 1.4125723\n",
      "4_2 train_acc: 0.3034 train_loss: 2.030755\tval_acc: 0.313725 val_loss: 1.3428400\n",
      "4_3 train_acc: 0.2996 train_loss: 1.899543\tval_acc: 0.357843 val_loss: 1.2774699\n",
      "4_4 train_acc: 0.3109 train_loss: 1.887175\tval_acc: 0.411765 val_loss: 1.2511839\n",
      "4_5 train_acc: 0.3783 train_loss: 1.761443\tval_acc: 0.460784 val_loss: 1.2273276\n",
      "4_6 train_acc: 0.3296 train_loss: 1.716038\tval_acc: 0.475490 val_loss: 1.2015167\n",
      "4_7 train_acc: 0.3670 train_loss: 1.571050\tval_acc: 0.519608 val_loss: 1.1588602\n",
      "4_8 train_acc: 0.3483 train_loss: 1.562182\tval_acc: 0.549020 val_loss: 1.1263535\n",
      "4_9 train_acc: 0.3933 train_loss: 1.539194\tval_acc: 0.602941 val_loss: 1.1021215\n",
      "4_10 train_acc: 0.3783 train_loss: 1.533491\tval_acc: 0.656863 val_loss: 1.0629258\n",
      "4_12 train_acc: 0.4120 train_loss: 1.414757\tval_acc: 0.656863 val_loss: 1.0089996\n",
      "4_14 train_acc: 0.4232 train_loss: 1.357786\tval_acc: 0.651961 val_loss: 0.9938569\n",
      "4_15 train_acc: 0.3820 train_loss: 1.405862\tval_acc: 0.710784 val_loss: 0.9764493\n",
      "4_16 train_acc: 0.4157 train_loss: 1.327655\tval_acc: 0.671569 val_loss: 0.9468207\n",
      "4_17 train_acc: 0.4869 train_loss: 1.167836\tval_acc: 0.715686 val_loss: 0.9063425\n",
      "4_20 train_acc: 0.4757 train_loss: 1.207686\tval_acc: 0.705882 val_loss: 0.8798975\n",
      "4_21 train_acc: 0.4981 train_loss: 1.134944\tval_acc: 0.710784 val_loss: 0.8572364\n",
      "4_22 train_acc: 0.5206 train_loss: 1.128503\tval_acc: 0.730392 val_loss: 0.8419767\n",
      "4_23 train_acc: 0.5169 train_loss: 1.084936\tval_acc: 0.725490 val_loss: 0.8207335\n",
      "4_24 train_acc: 0.5206 train_loss: 1.032371\tval_acc: 0.745098 val_loss: 0.7959017\n",
      "4_25 train_acc: 0.5094 train_loss: 1.158594\tval_acc: 0.769608 val_loss: 0.7523608\n",
      "4_26 train_acc: 0.5918 train_loss: 1.060669\tval_acc: 0.764706 val_loss: 0.7510114\n",
      "4_27 train_acc: 0.5506 train_loss: 1.007100\tval_acc: 0.774510 val_loss: 0.7253768\n",
      "4_28 train_acc: 0.5918 train_loss: 0.984022\tval_acc: 0.779412 val_loss: 0.7208574\n",
      "4_29 train_acc: 0.5805 train_loss: 1.039988\tval_acc: 0.784314 val_loss: 0.6881188\n",
      "4_32 train_acc: 0.6105 train_loss: 0.894526\tval_acc: 0.784314 val_loss: 0.6595944\n",
      "4_34 train_acc: 0.6217 train_loss: 0.939978\tval_acc: 0.803922 val_loss: 0.6375252\n",
      "4_38 train_acc: 0.6180 train_loss: 0.891825\tval_acc: 0.794118 val_loss: 0.5868121\n",
      "4_42 train_acc: 0.6180 train_loss: 0.897768\tval_acc: 0.803922 val_loss: 0.5731006\n",
      "4_46 train_acc: 0.6891 train_loss: 0.813077\tval_acc: 0.789216 val_loss: 0.5570918\n",
      "4_49 train_acc: 0.6629 train_loss: 0.853283\tval_acc: 0.828431 val_loss: 0.5175903\n",
      "4_52 train_acc: 0.7004 train_loss: 0.744252\tval_acc: 0.813725 val_loss: 0.5124482\n",
      "4_53 train_acc: 0.6479 train_loss: 0.840681\tval_acc: 0.848039 val_loss: 0.4976341\n",
      "4_56 train_acc: 0.6554 train_loss: 0.826383\tval_acc: 0.823529 val_loss: 0.4867741\n",
      "4_57 train_acc: 0.6517 train_loss: 0.888110\tval_acc: 0.843137 val_loss: 0.4860609\n",
      "4_58 train_acc: 0.6854 train_loss: 0.794288\tval_acc: 0.828431 val_loss: 0.4744095\n",
      "4_60 train_acc: 0.7004 train_loss: 0.765645\tval_acc: 0.848039 val_loss: 0.4699218\n",
      "4_61 train_acc: 0.7341 train_loss: 0.643574\tval_acc: 0.833333 val_loss: 0.4531853\n",
      "4_62 train_acc: 0.7228 train_loss: 0.692911\tval_acc: 0.833333 val_loss: 0.4529531\n",
      "4_63 train_acc: 0.7154 train_loss: 0.750194\tval_acc: 0.857843 val_loss: 0.4417357\n",
      "4_65 train_acc: 0.7228 train_loss: 0.713759\tval_acc: 0.857843 val_loss: 0.4287384\n",
      "4_67 train_acc: 0.7416 train_loss: 0.688240\tval_acc: 0.857843 val_loss: 0.4276487\n",
      "4_70 train_acc: 0.7303 train_loss: 0.742000\tval_acc: 0.867647 val_loss: 0.4096945\n",
      "4_73 train_acc: 0.7004 train_loss: 0.759305\tval_acc: 0.892157 val_loss: 0.3842776\n",
      "4_85 train_acc: 0.7303 train_loss: 0.632561\tval_acc: 0.872549 val_loss: 0.3785999\n",
      "4_86 train_acc: 0.7266 train_loss: 0.683761\tval_acc: 0.877451 val_loss: 0.3591721\n",
      "4_94 train_acc: 0.7416 train_loss: 0.608497\tval_acc: 0.901961 val_loss: 0.3408108\n",
      "4_95 train_acc: 0.6929 train_loss: 0.690894\tval_acc: 0.916667 val_loss: 0.3214407\n",
      "4_96 train_acc: 0.7528 train_loss: 0.655280\tval_acc: 0.911765 val_loss: 0.3131976\n",
      "4_108 train_acc: 0.7978 train_loss: 0.558565\tval_acc: 0.901961 val_loss: 0.2952658\n",
      "4_110 train_acc: 0.7341 train_loss: 0.666252\tval_acc: 0.901961 val_loss: 0.2914729\n",
      "4_115 train_acc: 0.7678 train_loss: 0.640090\tval_acc: 0.911765 val_loss: 0.2876600\n",
      "4_117 train_acc: 0.7528 train_loss: 0.592964\tval_acc: 0.916667 val_loss: 0.2655656\n",
      "4_125 train_acc: 0.7640 train_loss: 0.643168\tval_acc: 0.946078 val_loss: 0.2482704\n",
      "4_136 train_acc: 0.7416 train_loss: 0.638983\tval_acc: 0.926471 val_loss: 0.2309402\n",
      "4_144 train_acc: 0.7416 train_loss: 0.587212\tval_acc: 0.955882 val_loss: 0.2179919\n",
      "4_154 train_acc: 0.8127 train_loss: 0.507202\tval_acc: 0.931373 val_loss: 0.2020090\n",
      "4_168 train_acc: 0.7940 train_loss: 0.505905\tval_acc: 0.941176 val_loss: 0.1893459\n",
      "4_173 train_acc: 0.8614 train_loss: 0.423325\tval_acc: 0.955882 val_loss: 0.1823575\n",
      "4_176 train_acc: 0.7828 train_loss: 0.541247\tval_acc: 0.955882 val_loss: 0.1795439\n",
      "4_187 train_acc: 0.8165 train_loss: 0.463473\tval_acc: 0.955882 val_loss: 0.1580729\n",
      "4_210 train_acc: 0.8427 train_loss: 0.411681\tval_acc: 0.955882 val_loss: 0.1569741\n",
      "4_220 train_acc: 0.8390 train_loss: 0.452885\tval_acc: 0.970588 val_loss: 0.1517992\n",
      "4_222 train_acc: 0.7903 train_loss: 0.502534\tval_acc: 0.960784 val_loss: 0.1501269\n",
      "4_228 train_acc: 0.8839 train_loss: 0.327126\tval_acc: 0.965686 val_loss: 0.1490218\n",
      "4_230 train_acc: 0.8352 train_loss: 0.409767\tval_acc: 0.960784 val_loss: 0.1426972\n",
      "4_236 train_acc: 0.8277 train_loss: 0.461502\tval_acc: 0.950980 val_loss: 0.1408117\n",
      "4_239 train_acc: 0.8202 train_loss: 0.520371\tval_acc: 0.975490 val_loss: 0.1195632\n",
      "4_270 train_acc: 0.8427 train_loss: 0.380326\tval_acc: 0.980392 val_loss: 0.1028731\n",
      "4_284 train_acc: 0.8090 train_loss: 0.458764\tval_acc: 0.980392 val_loss: 0.1009982\n",
      "4_294 train_acc: 0.8352 train_loss: 0.393757\tval_acc: 0.985294 val_loss: 0.0941648\n",
      "4_322 train_acc: 0.8352 train_loss: 0.427212\tval_acc: 0.975490 val_loss: 0.0936404\n",
      "4_331 train_acc: 0.8315 train_loss: 0.404957\tval_acc: 0.985294 val_loss: 0.0931496\n",
      "4_340 train_acc: 0.8390 train_loss: 0.414458\tval_acc: 0.970588 val_loss: 0.0896096\n",
      "4_345 train_acc: 0.8614 train_loss: 0.374665\tval_acc: 0.980392 val_loss: 0.0809564\n",
      "4_347 train_acc: 0.8240 train_loss: 0.453036\tval_acc: 0.980392 val_loss: 0.0809328\n",
      "4_358 train_acc: 0.8577 train_loss: 0.341733\tval_acc: 0.980392 val_loss: 0.0769410\n",
      "4_380 train_acc: 0.8577 train_loss: 0.351990\tval_acc: 0.990196 val_loss: 0.0768805\n",
      "4_381 train_acc: 0.8090 train_loss: 0.386985\tval_acc: 0.995098 val_loss: 0.0678658\n",
      "4_398 train_acc: 0.8801 train_loss: 0.362093\tval_acc: 0.995098 val_loss: 0.0636026\n",
      "4_405 train_acc: 0.8502 train_loss: 0.414199\tval_acc: 0.985294 val_loss: 0.0615936\n",
      "4_408 train_acc: 0.8689 train_loss: 0.316563\tval_acc: 0.990196 val_loss: 0.0558805\n",
      "4_488 train_acc: 0.8614 train_loss: 0.398250\tval_acc: 0.990196 val_loss: 0.0517542\n",
      "4_489 train_acc: 0.8427 train_loss: 0.386481\tval_acc: 0.990196 val_loss: 0.0510101\n",
      "4_501 train_acc: 0.8764 train_loss: 0.305518\tval_acc: 1.000000 val_loss: 0.0475469\n",
      "4_516 train_acc: 0.8464 train_loss: 0.383148\tval_acc: 0.985294 val_loss: 0.0469918\n",
      "4_529 train_acc: 0.9139 train_loss: 0.244048\tval_acc: 0.995098 val_loss: 0.0467741\n",
      "4_536 train_acc: 0.8989 train_loss: 0.330867\tval_acc: 1.000000 val_loss: 0.0433240\n",
      "4_552 train_acc: 0.8839 train_loss: 0.317929\tval_acc: 0.995098 val_loss: 0.0425202\n",
      "4_570 train_acc: 0.8577 train_loss: 0.359198\tval_acc: 1.000000 val_loss: 0.0367334\n",
      "4_614 train_acc: 0.8577 train_loss: 0.355145\tval_acc: 1.000000 val_loss: 0.0350892\n",
      "4_615 train_acc: 0.8839 train_loss: 0.278483\tval_acc: 1.000000 val_loss: 0.0283932\n",
      "4_649 train_acc: 0.9064 train_loss: 0.253041\tval_acc: 1.000000 val_loss: 0.0245943\n",
      "4_697 train_acc: 0.8801 train_loss: 0.307801\tval_acc: 1.000000 val_loss: 0.0223216\n",
      "4_777 train_acc: 0.8989 train_loss: 0.239765\tval_acc: 1.000000 val_loss: 0.0175570\n",
      "4_850 train_acc: 0.8876 train_loss: 0.315031\tval_acc: 1.000000 val_loss: 0.0174471\n",
      "4_875 train_acc: 0.9326 train_loss: 0.201102\tval_acc: 1.000000 val_loss: 0.0171211\n",
      "4_885 train_acc: 0.9101 train_loss: 0.262551\tval_acc: 1.000000 val_loss: 0.0169050\n",
      "4_958 train_acc: 0.9139 train_loss: 0.193946\tval_acc: 1.000000 val_loss: 0.0147353\n",
      "4_979 train_acc: 0.8914 train_loss: 0.267401\tval_acc: 1.000000 val_loss: 0.0145738\n",
      "4_990 train_acc: 0.9064 train_loss: 0.317763\tval_acc: 1.000000 val_loss: 0.0130608\n",
      "epoch:  990 \tThe test accuracy is: 0.8472222222222222\n",
      " THE BEST ACCURACY IS 0.8472222222222222\tkappa is 0.7962962962962963\n",
      "subject 4 duration: 0:45:28.313225\n",
      "seed is 807\n",
      "Subject 5\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "5_0 train_acc: 0.2584 train_loss: 2.386393\tval_acc: 0.303922 val_loss: 1.3802462\n",
      "5_1 train_acc: 0.2921 train_loss: 2.085206\tval_acc: 0.338235 val_loss: 1.3168862\n",
      "5_2 train_acc: 0.3184 train_loss: 1.984489\tval_acc: 0.421569 val_loss: 1.2758901\n",
      "5_3 train_acc: 0.3333 train_loss: 1.876021\tval_acc: 0.431373 val_loss: 1.2571119\n",
      "5_4 train_acc: 0.2659 train_loss: 1.870903\tval_acc: 0.441176 val_loss: 1.2563447\n",
      "5_5 train_acc: 0.3184 train_loss: 1.774432\tval_acc: 0.480392 val_loss: 1.2131728\n",
      "5_6 train_acc: 0.3783 train_loss: 1.573552\tval_acc: 0.549020 val_loss: 1.1348963\n",
      "5_7 train_acc: 0.4082 train_loss: 1.466490\tval_acc: 0.607843 val_loss: 1.0809139\n",
      "5_8 train_acc: 0.3933 train_loss: 1.525160\tval_acc: 0.627451 val_loss: 1.0242325\n",
      "5_9 train_acc: 0.4345 train_loss: 1.341926\tval_acc: 0.666667 val_loss: 0.9776912\n",
      "5_10 train_acc: 0.3933 train_loss: 1.383390\tval_acc: 0.725490 val_loss: 0.9092541\n",
      "5_11 train_acc: 0.4906 train_loss: 1.343065\tval_acc: 0.754902 val_loss: 0.8338027\n",
      "5_12 train_acc: 0.5243 train_loss: 1.154985\tval_acc: 0.754902 val_loss: 0.8219203\n",
      "5_13 train_acc: 0.5019 train_loss: 1.241995\tval_acc: 0.794118 val_loss: 0.7599787\n",
      "5_14 train_acc: 0.5918 train_loss: 1.043269\tval_acc: 0.774510 val_loss: 0.7166151\n",
      "5_16 train_acc: 0.5993 train_loss: 0.979574\tval_acc: 0.764706 val_loss: 0.7019346\n",
      "5_17 train_acc: 0.6255 train_loss: 0.922362\tval_acc: 0.823529 val_loss: 0.6299168\n",
      "5_19 train_acc: 0.5993 train_loss: 0.966983\tval_acc: 0.803922 val_loss: 0.6148521\n",
      "5_20 train_acc: 0.6517 train_loss: 0.857997\tval_acc: 0.784314 val_loss: 0.6009067\n",
      "5_21 train_acc: 0.6255 train_loss: 0.887622\tval_acc: 0.803922 val_loss: 0.5773551\n",
      "5_24 train_acc: 0.6779 train_loss: 0.822088\tval_acc: 0.828431 val_loss: 0.5673412\n",
      "5_29 train_acc: 0.6479 train_loss: 0.821173\tval_acc: 0.808824 val_loss: 0.5436834\n",
      "5_30 train_acc: 0.6966 train_loss: 0.739189\tval_acc: 0.838235 val_loss: 0.5270975\n",
      "5_32 train_acc: 0.7116 train_loss: 0.706770\tval_acc: 0.833333 val_loss: 0.5214233\n",
      "5_33 train_acc: 0.7004 train_loss: 0.676010\tval_acc: 0.818627 val_loss: 0.5175955\n",
      "5_34 train_acc: 0.7266 train_loss: 0.709480\tval_acc: 0.843137 val_loss: 0.4977938\n",
      "5_35 train_acc: 0.6742 train_loss: 0.805428\tval_acc: 0.862745 val_loss: 0.4625284\n",
      "5_40 train_acc: 0.7303 train_loss: 0.700880\tval_acc: 0.852941 val_loss: 0.4412690\n",
      "5_44 train_acc: 0.7228 train_loss: 0.661021\tval_acc: 0.872549 val_loss: 0.4242110\n",
      "5_45 train_acc: 0.7453 train_loss: 0.669088\tval_acc: 0.882353 val_loss: 0.4031270\n",
      "5_48 train_acc: 0.7678 train_loss: 0.569538\tval_acc: 0.867647 val_loss: 0.3923337\n",
      "5_55 train_acc: 0.7865 train_loss: 0.531155\tval_acc: 0.882353 val_loss: 0.3651172\n",
      "5_57 train_acc: 0.7566 train_loss: 0.620313\tval_acc: 0.892157 val_loss: 0.3519782\n",
      "5_59 train_acc: 0.8352 train_loss: 0.430631\tval_acc: 0.911765 val_loss: 0.3470614\n",
      "5_60 train_acc: 0.7940 train_loss: 0.627408\tval_acc: 0.887255 val_loss: 0.3445434\n",
      "5_63 train_acc: 0.7416 train_loss: 0.687499\tval_acc: 0.892157 val_loss: 0.3372572\n",
      "5_65 train_acc: 0.7753 train_loss: 0.637046\tval_acc: 0.911765 val_loss: 0.3225663\n",
      "5_69 train_acc: 0.7903 train_loss: 0.557873\tval_acc: 0.901961 val_loss: 0.3122132\n",
      "5_73 train_acc: 0.7678 train_loss: 0.550569\tval_acc: 0.921569 val_loss: 0.2859858\n",
      "5_83 train_acc: 0.7678 train_loss: 0.603074\tval_acc: 0.926471 val_loss: 0.2591333\n",
      "5_89 train_acc: 0.8240 train_loss: 0.436836\tval_acc: 0.946078 val_loss: 0.2396249\n",
      "5_94 train_acc: 0.8614 train_loss: 0.380122\tval_acc: 0.921569 val_loss: 0.2334681\n",
      "5_111 train_acc: 0.8240 train_loss: 0.439603\tval_acc: 0.946078 val_loss: 0.1957492\n",
      "5_119 train_acc: 0.8390 train_loss: 0.463854\tval_acc: 0.946078 val_loss: 0.1917279\n",
      "5_120 train_acc: 0.8390 train_loss: 0.445732\tval_acc: 0.946078 val_loss: 0.1824356\n",
      "5_133 train_acc: 0.8689 train_loss: 0.397171\tval_acc: 0.946078 val_loss: 0.1707567\n",
      "5_153 train_acc: 0.8539 train_loss: 0.407061\tval_acc: 0.950980 val_loss: 0.1628447\n",
      "5_156 train_acc: 0.8202 train_loss: 0.424768\tval_acc: 0.960784 val_loss: 0.1481299\n",
      "5_164 train_acc: 0.8464 train_loss: 0.458252\tval_acc: 0.950980 val_loss: 0.1465355\n",
      "5_167 train_acc: 0.8352 train_loss: 0.450189\tval_acc: 0.950980 val_loss: 0.1438864\n",
      "5_176 train_acc: 0.8240 train_loss: 0.429216\tval_acc: 0.946078 val_loss: 0.1425881\n",
      "5_184 train_acc: 0.8202 train_loss: 0.429175\tval_acc: 0.950980 val_loss: 0.1237817\n",
      "5_199 train_acc: 0.8539 train_loss: 0.376741\tval_acc: 0.955882 val_loss: 0.1203452\n",
      "5_210 train_acc: 0.8539 train_loss: 0.394161\tval_acc: 0.960784 val_loss: 0.1095470\n",
      "5_219 train_acc: 0.8352 train_loss: 0.416181\tval_acc: 0.965686 val_loss: 0.1060118\n",
      "5_231 train_acc: 0.8652 train_loss: 0.355711\tval_acc: 0.975490 val_loss: 0.0952636\n",
      "5_246 train_acc: 0.8727 train_loss: 0.308357\tval_acc: 0.975490 val_loss: 0.0829390\n",
      "5_281 train_acc: 0.8390 train_loss: 0.357842\tval_acc: 0.990196 val_loss: 0.0804465\n",
      "5_287 train_acc: 0.8839 train_loss: 0.313912\tval_acc: 0.975490 val_loss: 0.0723749\n",
      "5_317 train_acc: 0.8876 train_loss: 0.280921\tval_acc: 0.980392 val_loss: 0.0621403\n",
      "5_426 train_acc: 0.9064 train_loss: 0.256054\tval_acc: 0.985294 val_loss: 0.0522364\n",
      "5_453 train_acc: 0.8989 train_loss: 0.269659\tval_acc: 0.995098 val_loss: 0.0438682\n",
      "5_486 train_acc: 0.8951 train_loss: 0.278112\tval_acc: 0.990196 val_loss: 0.0418363\n",
      "5_578 train_acc: 0.8876 train_loss: 0.257358\tval_acc: 0.995098 val_loss: 0.0364547\n",
      "5_581 train_acc: 0.9139 train_loss: 0.238798\tval_acc: 0.990196 val_loss: 0.0334430\n",
      "5_589 train_acc: 0.8876 train_loss: 0.344487\tval_acc: 1.000000 val_loss: 0.0312697\n",
      "5_708 train_acc: 0.9288 train_loss: 0.191067\tval_acc: 1.000000 val_loss: 0.0310249\n",
      "5_709 train_acc: 0.9326 train_loss: 0.229708\tval_acc: 0.995098 val_loss: 0.0269704\n",
      "5_769 train_acc: 0.9326 train_loss: 0.196793\tval_acc: 1.000000 val_loss: 0.0254119\n",
      "5_781 train_acc: 0.9213 train_loss: 0.228014\tval_acc: 0.995098 val_loss: 0.0246290\n",
      "5_845 train_acc: 0.9176 train_loss: 0.249749\tval_acc: 1.000000 val_loss: 0.0201137\n",
      "5_902 train_acc: 0.9438 train_loss: 0.167165\tval_acc: 1.000000 val_loss: 0.0200498\n",
      "5_926 train_acc: 0.9139 train_loss: 0.213633\tval_acc: 1.000000 val_loss: 0.0195373\n",
      "5_994 train_acc: 0.9213 train_loss: 0.196333\tval_acc: 1.000000 val_loss: 0.0176312\n",
      "epoch:  994 \tThe test accuracy is: 0.7534722222222222\n",
      " THE BEST ACCURACY IS 0.7534722222222222\tkappa is 0.6712962962962963\n",
      "subject 5 duration: 0:45:05.187535\n",
      "seed is 717\n",
      "Subject 6\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "6_0 train_acc: 0.2434 train_loss: 2.499294\tval_acc: 0.254902 val_loss: 1.3923926\n",
      "6_1 train_acc: 0.2734 train_loss: 2.305589\tval_acc: 0.318627 val_loss: 1.3472961\n",
      "6_2 train_acc: 0.2884 train_loss: 2.104728\tval_acc: 0.406863 val_loss: 1.2887514\n",
      "6_3 train_acc: 0.3184 train_loss: 1.826418\tval_acc: 0.495098 val_loss: 1.2344608\n",
      "6_4 train_acc: 0.3296 train_loss: 1.691014\tval_acc: 0.558824 val_loss: 1.1999134\n",
      "6_5 train_acc: 0.3333 train_loss: 1.772495\tval_acc: 0.509804 val_loss: 1.1667373\n",
      "6_6 train_acc: 0.3596 train_loss: 1.591637\tval_acc: 0.622549 val_loss: 1.1082366\n",
      "6_7 train_acc: 0.3596 train_loss: 1.558038\tval_acc: 0.627451 val_loss: 1.0752846\n",
      "6_8 train_acc: 0.4045 train_loss: 1.454453\tval_acc: 0.681373 val_loss: 1.0429488\n",
      "6_10 train_acc: 0.4120 train_loss: 1.317344\tval_acc: 0.681373 val_loss: 0.9634951\n",
      "6_11 train_acc: 0.3708 train_loss: 1.332535\tval_acc: 0.691176 val_loss: 0.9594988\n",
      "6_12 train_acc: 0.4457 train_loss: 1.224631\tval_acc: 0.700980 val_loss: 0.9064781\n",
      "6_13 train_acc: 0.4082 train_loss: 1.374826\tval_acc: 0.681373 val_loss: 0.8961121\n",
      "6_14 train_acc: 0.4494 train_loss: 1.234467\tval_acc: 0.720588 val_loss: 0.8614412\n",
      "6_15 train_acc: 0.5056 train_loss: 1.225555\tval_acc: 0.700980 val_loss: 0.8566902\n",
      "6_16 train_acc: 0.5056 train_loss: 1.147579\tval_acc: 0.696078 val_loss: 0.8443249\n",
      "6_18 train_acc: 0.4831 train_loss: 1.237496\tval_acc: 0.740196 val_loss: 0.8394670\n",
      "6_19 train_acc: 0.5094 train_loss: 1.105632\tval_acc: 0.735294 val_loss: 0.8377670\n",
      "6_20 train_acc: 0.5768 train_loss: 1.055596\tval_acc: 0.700980 val_loss: 0.8209816\n",
      "6_21 train_acc: 0.5506 train_loss: 1.066221\tval_acc: 0.750000 val_loss: 0.7510499\n",
      "6_26 train_acc: 0.5169 train_loss: 1.079284\tval_acc: 0.759804 val_loss: 0.7404504\n",
      "6_27 train_acc: 0.5843 train_loss: 0.970811\tval_acc: 0.750000 val_loss: 0.7124397\n",
      "6_29 train_acc: 0.5768 train_loss: 0.932445\tval_acc: 0.764706 val_loss: 0.7073754\n",
      "6_32 train_acc: 0.6292 train_loss: 0.941524\tval_acc: 0.754902 val_loss: 0.6734529\n",
      "6_33 train_acc: 0.6030 train_loss: 0.963462\tval_acc: 0.774510 val_loss: 0.6600907\n",
      "6_37 train_acc: 0.6217 train_loss: 0.899049\tval_acc: 0.794118 val_loss: 0.6407819\n",
      "6_38 train_acc: 0.6629 train_loss: 0.848137\tval_acc: 0.794118 val_loss: 0.6290603\n",
      "6_39 train_acc: 0.6330 train_loss: 0.874666\tval_acc: 0.784314 val_loss: 0.6182986\n",
      "6_41 train_acc: 0.6255 train_loss: 0.861123\tval_acc: 0.794118 val_loss: 0.6141449\n",
      "6_42 train_acc: 0.6105 train_loss: 0.940068\tval_acc: 0.794118 val_loss: 0.5956634\n",
      "6_44 train_acc: 0.6704 train_loss: 0.795124\tval_acc: 0.803922 val_loss: 0.5868454\n",
      "6_45 train_acc: 0.6667 train_loss: 0.862966\tval_acc: 0.818627 val_loss: 0.5720580\n",
      "6_46 train_acc: 0.7041 train_loss: 0.774509\tval_acc: 0.823529 val_loss: 0.5701315\n",
      "6_47 train_acc: 0.6891 train_loss: 0.740296\tval_acc: 0.862745 val_loss: 0.5179016\n",
      "6_53 train_acc: 0.7079 train_loss: 0.764748\tval_acc: 0.848039 val_loss: 0.4964836\n",
      "6_57 train_acc: 0.6667 train_loss: 0.758880\tval_acc: 0.833333 val_loss: 0.4958690\n",
      "6_59 train_acc: 0.7041 train_loss: 0.743075\tval_acc: 0.838235 val_loss: 0.4643520\n",
      "6_62 train_acc: 0.6816 train_loss: 0.751287\tval_acc: 0.857843 val_loss: 0.4643477\n",
      "6_63 train_acc: 0.7116 train_loss: 0.720556\tval_acc: 0.862745 val_loss: 0.4525189\n",
      "6_67 train_acc: 0.7228 train_loss: 0.734545\tval_acc: 0.848039 val_loss: 0.4402229\n",
      "6_69 train_acc: 0.7491 train_loss: 0.674090\tval_acc: 0.852941 val_loss: 0.4312531\n",
      "6_73 train_acc: 0.6816 train_loss: 0.784501\tval_acc: 0.882353 val_loss: 0.4254045\n",
      "6_74 train_acc: 0.7116 train_loss: 0.695989\tval_acc: 0.882353 val_loss: 0.4179310\n",
      "6_80 train_acc: 0.7903 train_loss: 0.585329\tval_acc: 0.877451 val_loss: 0.3949347\n",
      "6_95 train_acc: 0.7528 train_loss: 0.707113\tval_acc: 0.872549 val_loss: 0.3898486\n",
      "6_96 train_acc: 0.7715 train_loss: 0.624207\tval_acc: 0.882353 val_loss: 0.3875450\n",
      "6_99 train_acc: 0.7191 train_loss: 0.730030\tval_acc: 0.872549 val_loss: 0.3853511\n",
      "6_101 train_acc: 0.7453 train_loss: 0.644325\tval_acc: 0.892157 val_loss: 0.3784479\n",
      "6_104 train_acc: 0.7566 train_loss: 0.637052\tval_acc: 0.892157 val_loss: 0.3730356\n",
      "6_109 train_acc: 0.7753 train_loss: 0.542607\tval_acc: 0.877451 val_loss: 0.3565097\n",
      "6_110 train_acc: 0.7191 train_loss: 0.621739\tval_acc: 0.931373 val_loss: 0.3182432\n",
      "6_142 train_acc: 0.7903 train_loss: 0.554688\tval_acc: 0.892157 val_loss: 0.3146042\n",
      "6_152 train_acc: 0.7715 train_loss: 0.597631\tval_acc: 0.906863 val_loss: 0.3077931\n",
      "6_153 train_acc: 0.7603 train_loss: 0.629970\tval_acc: 0.911765 val_loss: 0.2964698\n",
      "6_156 train_acc: 0.7640 train_loss: 0.596931\tval_acc: 0.921569 val_loss: 0.2919654\n",
      "6_158 train_acc: 0.7865 train_loss: 0.562990\tval_acc: 0.916667 val_loss: 0.2801810\n",
      "6_165 train_acc: 0.7828 train_loss: 0.554989\tval_acc: 0.906863 val_loss: 0.2764371\n",
      "6_169 train_acc: 0.7865 train_loss: 0.555487\tval_acc: 0.931373 val_loss: 0.2639687\n",
      "6_187 train_acc: 0.7790 train_loss: 0.550447\tval_acc: 0.936275 val_loss: 0.2599826\n",
      "6_195 train_acc: 0.8127 train_loss: 0.506234\tval_acc: 0.941176 val_loss: 0.2588685\n",
      "6_196 train_acc: 0.7603 train_loss: 0.605237\tval_acc: 0.946078 val_loss: 0.2337754\n",
      "6_205 train_acc: 0.7790 train_loss: 0.524811\tval_acc: 0.931373 val_loss: 0.2313634\n",
      "6_217 train_acc: 0.7940 train_loss: 0.544056\tval_acc: 0.911765 val_loss: 0.2298091\n",
      "6_218 train_acc: 0.7753 train_loss: 0.490792\tval_acc: 0.936275 val_loss: 0.2238949\n",
      "6_219 train_acc: 0.7903 train_loss: 0.568551\tval_acc: 0.941176 val_loss: 0.2131980\n",
      "6_220 train_acc: 0.7978 train_loss: 0.498266\tval_acc: 0.936275 val_loss: 0.2124758\n",
      "6_225 train_acc: 0.8277 train_loss: 0.399956\tval_acc: 0.941176 val_loss: 0.2071353\n",
      "6_227 train_acc: 0.7790 train_loss: 0.498839\tval_acc: 0.950980 val_loss: 0.1970130\n",
      "6_231 train_acc: 0.8352 train_loss: 0.469466\tval_acc: 0.941176 val_loss: 0.1940545\n",
      "6_240 train_acc: 0.8165 train_loss: 0.484096\tval_acc: 0.950980 val_loss: 0.1894628\n",
      "6_241 train_acc: 0.8277 train_loss: 0.438661\tval_acc: 0.955882 val_loss: 0.1847318\n",
      "6_246 train_acc: 0.7978 train_loss: 0.551779\tval_acc: 0.970588 val_loss: 0.1576108\n",
      "6_292 train_acc: 0.8090 train_loss: 0.466879\tval_acc: 0.960784 val_loss: 0.1566228\n",
      "6_296 train_acc: 0.8539 train_loss: 0.355149\tval_acc: 0.955882 val_loss: 0.1562806\n",
      "6_299 train_acc: 0.8352 train_loss: 0.413774\tval_acc: 0.955882 val_loss: 0.1555064\n",
      "6_306 train_acc: 0.8502 train_loss: 0.422062\tval_acc: 0.960784 val_loss: 0.1545932\n",
      "6_309 train_acc: 0.8202 train_loss: 0.450483\tval_acc: 0.965686 val_loss: 0.1525963\n",
      "6_316 train_acc: 0.8464 train_loss: 0.443966\tval_acc: 0.980392 val_loss: 0.1215273\n",
      "6_370 train_acc: 0.8727 train_loss: 0.365444\tval_acc: 0.970588 val_loss: 0.1137381\n",
      "6_430 train_acc: 0.8764 train_loss: 0.354865\tval_acc: 0.970588 val_loss: 0.0979708\n",
      "6_433 train_acc: 0.8764 train_loss: 0.358253\tval_acc: 0.985294 val_loss: 0.0962028\n",
      "6_437 train_acc: 0.8577 train_loss: 0.412713\tval_acc: 0.985294 val_loss: 0.0820049\n",
      "6_536 train_acc: 0.8502 train_loss: 0.414984\tval_acc: 0.985294 val_loss: 0.0764781\n",
      "6_560 train_acc: 0.8652 train_loss: 0.360808\tval_acc: 0.985294 val_loss: 0.0726824\n",
      "6_574 train_acc: 0.8727 train_loss: 0.340445\tval_acc: 0.985294 val_loss: 0.0708790\n",
      "6_638 train_acc: 0.8315 train_loss: 0.373188\tval_acc: 0.990196 val_loss: 0.0622601\n",
      "6_703 train_acc: 0.8876 train_loss: 0.289417\tval_acc: 0.985294 val_loss: 0.0610023\n",
      "6_711 train_acc: 0.8989 train_loss: 0.287877\tval_acc: 0.990196 val_loss: 0.0517707\n",
      "6_741 train_acc: 0.9176 train_loss: 0.240262\tval_acc: 0.985294 val_loss: 0.0488726\n",
      "6_800 train_acc: 0.9101 train_loss: 0.273889\tval_acc: 0.990196 val_loss: 0.0470215\n",
      "6_853 train_acc: 0.9176 train_loss: 0.246288\tval_acc: 0.995098 val_loss: 0.0436651\n",
      "6_931 train_acc: 0.8614 train_loss: 0.370299\tval_acc: 0.995098 val_loss: 0.0390149\n",
      "6_982 train_acc: 0.8801 train_loss: 0.329908\tval_acc: 0.990196 val_loss: 0.0389078\n",
      "epoch:  982 \tThe test accuracy is: 0.65625\n",
      " THE BEST ACCURACY IS 0.65625\tkappa is 0.5416666666666667\n",
      "subject 6 duration: 0:47:00.640036\n",
      "seed is 845\n",
      "Subject 7\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "7_0 train_acc: 0.2547 train_loss: 2.499434\tval_acc: 0.284314 val_loss: 1.4194660\n",
      "7_1 train_acc: 0.3071 train_loss: 1.965858\tval_acc: 0.416667 val_loss: 1.2918618\n",
      "7_2 train_acc: 0.3184 train_loss: 1.956573\tval_acc: 0.401961 val_loss: 1.2565967\n",
      "7_3 train_acc: 0.3221 train_loss: 1.741695\tval_acc: 0.495098 val_loss: 1.1447313\n",
      "7_4 train_acc: 0.3895 train_loss: 1.490077\tval_acc: 0.578431 val_loss: 1.0630610\n",
      "7_5 train_acc: 0.4195 train_loss: 1.387901\tval_acc: 0.671569 val_loss: 0.9810522\n",
      "7_6 train_acc: 0.4270 train_loss: 1.426876\tval_acc: 0.691176 val_loss: 0.9071059\n",
      "7_7 train_acc: 0.4831 train_loss: 1.301817\tval_acc: 0.715686 val_loss: 0.8174042\n",
      "7_8 train_acc: 0.5131 train_loss: 1.237307\tval_acc: 0.696078 val_loss: 0.8025423\n",
      "7_9 train_acc: 0.5543 train_loss: 1.186752\tval_acc: 0.720588 val_loss: 0.7579122\n",
      "7_10 train_acc: 0.5543 train_loss: 1.169574\tval_acc: 0.720588 val_loss: 0.7349998\n",
      "7_11 train_acc: 0.4981 train_loss: 1.161456\tval_acc: 0.759804 val_loss: 0.7207869\n",
      "7_12 train_acc: 0.5918 train_loss: 0.967118\tval_acc: 0.769608 val_loss: 0.6681430\n",
      "7_15 train_acc: 0.6217 train_loss: 0.928232\tval_acc: 0.754902 val_loss: 0.6432417\n",
      "7_16 train_acc: 0.6592 train_loss: 0.875813\tval_acc: 0.769608 val_loss: 0.6012500\n",
      "7_19 train_acc: 0.6517 train_loss: 0.876746\tval_acc: 0.779412 val_loss: 0.5562084\n",
      "7_22 train_acc: 0.6479 train_loss: 0.839201\tval_acc: 0.799020 val_loss: 0.5516534\n",
      "7_23 train_acc: 0.7079 train_loss: 0.769191\tval_acc: 0.808824 val_loss: 0.5298411\n",
      "7_24 train_acc: 0.6779 train_loss: 0.862399\tval_acc: 0.843137 val_loss: 0.4884353\n",
      "7_27 train_acc: 0.6854 train_loss: 0.766262\tval_acc: 0.833333 val_loss: 0.4847275\n",
      "7_30 train_acc: 0.7491 train_loss: 0.636460\tval_acc: 0.848039 val_loss: 0.4731414\n",
      "7_31 train_acc: 0.7191 train_loss: 0.724754\tval_acc: 0.887255 val_loss: 0.4301425\n",
      "7_32 train_acc: 0.6929 train_loss: 0.708473\tval_acc: 0.852941 val_loss: 0.4220513\n",
      "7_37 train_acc: 0.6929 train_loss: 0.666962\tval_acc: 0.887255 val_loss: 0.3608983\n",
      "7_46 train_acc: 0.7566 train_loss: 0.692329\tval_acc: 0.892157 val_loss: 0.3500074\n",
      "7_47 train_acc: 0.8240 train_loss: 0.485156\tval_acc: 0.921569 val_loss: 0.3073627\n",
      "7_57 train_acc: 0.7678 train_loss: 0.564541\tval_acc: 0.901961 val_loss: 0.2881737\n",
      "7_65 train_acc: 0.8015 train_loss: 0.521509\tval_acc: 0.901961 val_loss: 0.2832641\n",
      "7_66 train_acc: 0.8127 train_loss: 0.487994\tval_acc: 0.897059 val_loss: 0.2547348\n",
      "7_70 train_acc: 0.7940 train_loss: 0.574695\tval_acc: 0.931373 val_loss: 0.2514102\n",
      "7_76 train_acc: 0.8277 train_loss: 0.410685\tval_acc: 0.906863 val_loss: 0.2397862\n",
      "7_80 train_acc: 0.8127 train_loss: 0.469393\tval_acc: 0.936275 val_loss: 0.2163451\n",
      "7_85 train_acc: 0.8464 train_loss: 0.412727\tval_acc: 0.936275 val_loss: 0.1932876\n",
      "7_96 train_acc: 0.8652 train_loss: 0.365094\tval_acc: 0.946078 val_loss: 0.1803252\n",
      "7_102 train_acc: 0.8127 train_loss: 0.480900\tval_acc: 0.950980 val_loss: 0.1672445\n",
      "7_117 train_acc: 0.8989 train_loss: 0.318124\tval_acc: 0.946078 val_loss: 0.1517633\n",
      "7_130 train_acc: 0.8539 train_loss: 0.430681\tval_acc: 0.950980 val_loss: 0.1487869\n",
      "7_135 train_acc: 0.8727 train_loss: 0.356872\tval_acc: 0.936275 val_loss: 0.1474752\n",
      "7_136 train_acc: 0.8839 train_loss: 0.337706\tval_acc: 0.960784 val_loss: 0.1369207\n",
      "7_145 train_acc: 0.9176 train_loss: 0.259447\tval_acc: 0.965686 val_loss: 0.0953495\n",
      "7_153 train_acc: 0.9213 train_loss: 0.228668\tval_acc: 0.975490 val_loss: 0.0704913\n",
      "7_157 train_acc: 0.9064 train_loss: 0.239619\tval_acc: 0.985294 val_loss: 0.0625651\n",
      "7_168 train_acc: 0.9213 train_loss: 0.188210\tval_acc: 0.985294 val_loss: 0.0561393\n",
      "7_175 train_acc: 0.9213 train_loss: 0.165965\tval_acc: 0.990196 val_loss: 0.0480864\n",
      "7_181 train_acc: 0.9438 train_loss: 0.191142\tval_acc: 0.990196 val_loss: 0.0460276\n",
      "7_182 train_acc: 0.9438 train_loss: 0.155843\tval_acc: 0.990196 val_loss: 0.0452507\n",
      "7_190 train_acc: 0.9513 train_loss: 0.142608\tval_acc: 0.985294 val_loss: 0.0423497\n",
      "7_196 train_acc: 0.9213 train_loss: 0.211319\tval_acc: 0.985294 val_loss: 0.0397585\n",
      "7_216 train_acc: 0.9213 train_loss: 0.182050\tval_acc: 0.995098 val_loss: 0.0380342\n",
      "7_220 train_acc: 0.9401 train_loss: 0.164033\tval_acc: 0.995098 val_loss: 0.0287462\n",
      "7_248 train_acc: 0.9438 train_loss: 0.163511\tval_acc: 0.995098 val_loss: 0.0274658\n",
      "7_266 train_acc: 0.9513 train_loss: 0.129981\tval_acc: 0.995098 val_loss: 0.0270659\n",
      "7_267 train_acc: 0.9551 train_loss: 0.117719\tval_acc: 1.000000 val_loss: 0.0220629\n",
      "7_329 train_acc: 0.9363 train_loss: 0.163466\tval_acc: 0.995098 val_loss: 0.0185279\n",
      "7_352 train_acc: 0.9700 train_loss: 0.089861\tval_acc: 0.990196 val_loss: 0.0179089\n",
      "7_414 train_acc: 0.9625 train_loss: 0.123550\tval_acc: 0.995098 val_loss: 0.0169483\n",
      "7_443 train_acc: 0.9551 train_loss: 0.113863\tval_acc: 0.995098 val_loss: 0.0169192\n",
      "7_449 train_acc: 0.9775 train_loss: 0.064597\tval_acc: 1.000000 val_loss: 0.0135305\n",
      "7_451 train_acc: 0.9625 train_loss: 0.111573\tval_acc: 1.000000 val_loss: 0.0106053\n",
      "7_506 train_acc: 0.9551 train_loss: 0.139455\tval_acc: 1.000000 val_loss: 0.0100322\n",
      "7_518 train_acc: 0.9551 train_loss: 0.093952\tval_acc: 1.000000 val_loss: 0.0091109\n",
      "7_522 train_acc: 0.9588 train_loss: 0.126295\tval_acc: 1.000000 val_loss: 0.0080096\n",
      "7_528 train_acc: 0.9551 train_loss: 0.106131\tval_acc: 1.000000 val_loss: 0.0073963\n",
      "7_610 train_acc: 0.9588 train_loss: 0.102557\tval_acc: 1.000000 val_loss: 0.0059805\n",
      "7_660 train_acc: 0.9513 train_loss: 0.099093\tval_acc: 1.000000 val_loss: 0.0047527\n",
      "7_771 train_acc: 0.9625 train_loss: 0.078996\tval_acc: 1.000000 val_loss: 0.0036168\n",
      "7_790 train_acc: 0.9625 train_loss: 0.101404\tval_acc: 1.000000 val_loss: 0.0034319\n",
      "7_931 train_acc: 0.9663 train_loss: 0.095048\tval_acc: 1.000000 val_loss: 0.0029341\n",
      "7_941 train_acc: 0.9663 train_loss: 0.068081\tval_acc: 1.000000 val_loss: 0.0026597\n",
      "epoch:  941 \tThe test accuracy is: 0.90625\n",
      " THE BEST ACCURACY IS 0.90625\tkappa is 0.875\n",
      "subject 7 duration: 0:47:01.058444\n",
      "seed is 128\n",
      "Subject 8\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "8_0 train_acc: 0.2697 train_loss: 2.300021\tval_acc: 0.259804 val_loss: 1.5802176\n",
      "8_1 train_acc: 0.2809 train_loss: 2.160205\tval_acc: 0.274510 val_loss: 1.4249932\n",
      "8_2 train_acc: 0.3670 train_loss: 1.845121\tval_acc: 0.308824 val_loss: 1.3277868\n",
      "8_3 train_acc: 0.3745 train_loss: 1.846833\tval_acc: 0.313725 val_loss: 1.2795792\n",
      "8_4 train_acc: 0.3558 train_loss: 1.731159\tval_acc: 0.524510 val_loss: 1.1341072\n",
      "8_5 train_acc: 0.3820 train_loss: 1.630803\tval_acc: 0.602941 val_loss: 1.0593716\n",
      "8_6 train_acc: 0.3446 train_loss: 1.655540\tval_acc: 0.656863 val_loss: 1.0082536\n",
      "8_7 train_acc: 0.3858 train_loss: 1.459896\tval_acc: 0.632353 val_loss: 0.9758384\n",
      "8_8 train_acc: 0.4382 train_loss: 1.380849\tval_acc: 0.632353 val_loss: 0.9592742\n",
      "8_9 train_acc: 0.4944 train_loss: 1.355663\tval_acc: 0.671569 val_loss: 0.8924802\n",
      "8_10 train_acc: 0.4457 train_loss: 1.313875\tval_acc: 0.735294 val_loss: 0.8850713\n",
      "8_11 train_acc: 0.4419 train_loss: 1.309437\tval_acc: 0.735294 val_loss: 0.8277245\n",
      "8_13 train_acc: 0.5131 train_loss: 1.147603\tval_acc: 0.740196 val_loss: 0.7960154\n",
      "8_15 train_acc: 0.5318 train_loss: 1.101760\tval_acc: 0.754902 val_loss: 0.7506787\n",
      "8_17 train_acc: 0.5094 train_loss: 1.121130\tval_acc: 0.730392 val_loss: 0.7499719\n",
      "8_18 train_acc: 0.5918 train_loss: 1.001949\tval_acc: 0.735294 val_loss: 0.7373201\n",
      "8_19 train_acc: 0.5393 train_loss: 1.046343\tval_acc: 0.730392 val_loss: 0.7358340\n",
      "8_20 train_acc: 0.5918 train_loss: 1.005110\tval_acc: 0.725490 val_loss: 0.7193680\n",
      "8_21 train_acc: 0.6067 train_loss: 0.895005\tval_acc: 0.754902 val_loss: 0.7032638\n",
      "8_22 train_acc: 0.6854 train_loss: 0.838354\tval_acc: 0.789216 val_loss: 0.6729698\n",
      "8_23 train_acc: 0.6442 train_loss: 0.876397\tval_acc: 0.774510 val_loss: 0.6397097\n",
      "8_24 train_acc: 0.6292 train_loss: 0.903391\tval_acc: 0.794118 val_loss: 0.6369376\n",
      "8_26 train_acc: 0.6180 train_loss: 0.871483\tval_acc: 0.764706 val_loss: 0.6275885\n",
      "8_27 train_acc: 0.6854 train_loss: 0.763066\tval_acc: 0.779412 val_loss: 0.6201202\n",
      "8_28 train_acc: 0.6517 train_loss: 0.786423\tval_acc: 0.794118 val_loss: 0.5926321\n",
      "8_29 train_acc: 0.7378 train_loss: 0.719108\tval_acc: 0.823529 val_loss: 0.5548888\n",
      "8_31 train_acc: 0.6592 train_loss: 0.790363\tval_acc: 0.833333 val_loss: 0.5508534\n",
      "8_32 train_acc: 0.7154 train_loss: 0.760810\tval_acc: 0.833333 val_loss: 0.4887421\n",
      "8_36 train_acc: 0.7566 train_loss: 0.594562\tval_acc: 0.852941 val_loss: 0.4580491\n",
      "8_38 train_acc: 0.6816 train_loss: 0.760218\tval_acc: 0.877451 val_loss: 0.4474301\n",
      "8_39 train_acc: 0.7041 train_loss: 0.782973\tval_acc: 0.877451 val_loss: 0.4088930\n",
      "8_44 train_acc: 0.7715 train_loss: 0.594447\tval_acc: 0.872549 val_loss: 0.3770802\n",
      "8_45 train_acc: 0.7341 train_loss: 0.629327\tval_acc: 0.877451 val_loss: 0.3714993\n",
      "8_48 train_acc: 0.8240 train_loss: 0.480176\tval_acc: 0.882353 val_loss: 0.3603172\n",
      "8_51 train_acc: 0.7566 train_loss: 0.615599\tval_acc: 0.882353 val_loss: 0.3598105\n",
      "8_52 train_acc: 0.7978 train_loss: 0.489665\tval_acc: 0.877451 val_loss: 0.3559067\n",
      "8_53 train_acc: 0.7453 train_loss: 0.650672\tval_acc: 0.901961 val_loss: 0.3408963\n",
      "8_55 train_acc: 0.7790 train_loss: 0.507635\tval_acc: 0.887255 val_loss: 0.3249345\n",
      "8_59 train_acc: 0.8165 train_loss: 0.544405\tval_acc: 0.892157 val_loss: 0.3219992\n",
      "8_60 train_acc: 0.7715 train_loss: 0.578912\tval_acc: 0.906863 val_loss: 0.3048035\n",
      "8_62 train_acc: 0.7828 train_loss: 0.600180\tval_acc: 0.916667 val_loss: 0.2883496\n",
      "8_66 train_acc: 0.7715 train_loss: 0.569347\tval_acc: 0.916667 val_loss: 0.2793533\n",
      "8_71 train_acc: 0.8127 train_loss: 0.490322\tval_acc: 0.921569 val_loss: 0.2763891\n",
      "8_74 train_acc: 0.8202 train_loss: 0.449222\tval_acc: 0.911765 val_loss: 0.2685512\n",
      "8_75 train_acc: 0.8052 train_loss: 0.497959\tval_acc: 0.931373 val_loss: 0.2478006\n",
      "8_92 train_acc: 0.8052 train_loss: 0.501586\tval_acc: 0.941176 val_loss: 0.2384911\n",
      "8_94 train_acc: 0.8127 train_loss: 0.538284\tval_acc: 0.931373 val_loss: 0.2315252\n",
      "8_96 train_acc: 0.8652 train_loss: 0.388163\tval_acc: 0.931373 val_loss: 0.2142361\n",
      "8_115 train_acc: 0.8614 train_loss: 0.362353\tval_acc: 0.941176 val_loss: 0.2078143\n",
      "8_116 train_acc: 0.8427 train_loss: 0.426174\tval_acc: 0.936275 val_loss: 0.2064222\n",
      "8_124 train_acc: 0.8614 train_loss: 0.384088\tval_acc: 0.936275 val_loss: 0.1940988\n",
      "8_128 train_acc: 0.8427 train_loss: 0.392324\tval_acc: 0.941176 val_loss: 0.1802034\n",
      "8_131 train_acc: 0.8165 train_loss: 0.476185\tval_acc: 0.960784 val_loss: 0.1697257\n",
      "8_138 train_acc: 0.8727 train_loss: 0.355188\tval_acc: 0.950980 val_loss: 0.1548493\n",
      "8_159 train_acc: 0.8652 train_loss: 0.367180\tval_acc: 0.970588 val_loss: 0.1462248\n",
      "8_170 train_acc: 0.8427 train_loss: 0.361661\tval_acc: 0.950980 val_loss: 0.1455242\n",
      "8_171 train_acc: 0.8689 train_loss: 0.314196\tval_acc: 0.965686 val_loss: 0.1342971\n",
      "8_173 train_acc: 0.8277 train_loss: 0.465243\tval_acc: 0.975490 val_loss: 0.1260815\n",
      "8_175 train_acc: 0.8577 train_loss: 0.372133\tval_acc: 0.975490 val_loss: 0.1226338\n",
      "8_194 train_acc: 0.8352 train_loss: 0.342706\tval_acc: 0.965686 val_loss: 0.1176007\n",
      "8_196 train_acc: 0.8876 train_loss: 0.307345\tval_acc: 0.970588 val_loss: 0.1148318\n",
      "8_197 train_acc: 0.8539 train_loss: 0.334050\tval_acc: 0.970588 val_loss: 0.0980164\n",
      "8_200 train_acc: 0.8839 train_loss: 0.278931\tval_acc: 0.975490 val_loss: 0.0970552\n",
      "8_204 train_acc: 0.8689 train_loss: 0.324848\tval_acc: 0.975490 val_loss: 0.0863761\n",
      "8_224 train_acc: 0.8914 train_loss: 0.266999\tval_acc: 0.975490 val_loss: 0.0846689\n",
      "8_226 train_acc: 0.8951 train_loss: 0.281238\tval_acc: 0.980392 val_loss: 0.0794897\n",
      "8_227 train_acc: 0.9026 train_loss: 0.305643\tval_acc: 0.980392 val_loss: 0.0737369\n",
      "8_236 train_acc: 0.8801 train_loss: 0.315230\tval_acc: 0.975490 val_loss: 0.0723482\n",
      "8_244 train_acc: 0.8951 train_loss: 0.304643\tval_acc: 0.990196 val_loss: 0.0667553\n",
      "8_248 train_acc: 0.8614 train_loss: 0.311712\tval_acc: 0.990196 val_loss: 0.0606200\n",
      "8_261 train_acc: 0.9476 train_loss: 0.171477\tval_acc: 0.985294 val_loss: 0.0565204\n",
      "8_279 train_acc: 0.9064 train_loss: 0.313837\tval_acc: 0.980392 val_loss: 0.0546702\n",
      "8_286 train_acc: 0.9101 train_loss: 0.238851\tval_acc: 0.985294 val_loss: 0.0492141\n",
      "8_288 train_acc: 0.9176 train_loss: 0.218443\tval_acc: 0.990196 val_loss: 0.0464184\n",
      "8_317 train_acc: 0.9251 train_loss: 0.236230\tval_acc: 0.980392 val_loss: 0.0424745\n",
      "8_354 train_acc: 0.9101 train_loss: 0.232052\tval_acc: 0.990196 val_loss: 0.0405484\n",
      "8_364 train_acc: 0.9213 train_loss: 0.216872\tval_acc: 0.985294 val_loss: 0.0386321\n",
      "8_365 train_acc: 0.9176 train_loss: 0.210280\tval_acc: 0.995098 val_loss: 0.0339266\n",
      "8_386 train_acc: 0.9251 train_loss: 0.232472\tval_acc: 0.990196 val_loss: 0.0328013\n",
      "8_410 train_acc: 0.9401 train_loss: 0.172339\tval_acc: 0.990196 val_loss: 0.0257073\n",
      "8_415 train_acc: 0.9326 train_loss: 0.201533\tval_acc: 0.990196 val_loss: 0.0256044\n",
      "8_450 train_acc: 0.9288 train_loss: 0.188836\tval_acc: 0.995098 val_loss: 0.0249348\n",
      "8_468 train_acc: 0.9288 train_loss: 0.191418\tval_acc: 0.995098 val_loss: 0.0205502\n",
      "8_500 train_acc: 0.9251 train_loss: 0.204916\tval_acc: 1.000000 val_loss: 0.0188281\n",
      "8_536 train_acc: 0.9213 train_loss: 0.161772\tval_acc: 1.000000 val_loss: 0.0157075\n",
      "8_571 train_acc: 0.9326 train_loss: 0.202407\tval_acc: 1.000000 val_loss: 0.0156029\n",
      "8_613 train_acc: 0.9401 train_loss: 0.149617\tval_acc: 0.995098 val_loss: 0.0144310\n",
      "8_625 train_acc: 0.9363 train_loss: 0.163758\tval_acc: 1.000000 val_loss: 0.0105275\n",
      "8_704 train_acc: 0.9401 train_loss: 0.144290\tval_acc: 1.000000 val_loss: 0.0102860\n",
      "8_713 train_acc: 0.9513 train_loss: 0.138589\tval_acc: 1.000000 val_loss: 0.0084489\n",
      "8_754 train_acc: 0.9513 train_loss: 0.127197\tval_acc: 1.000000 val_loss: 0.0075277\n",
      "8_809 train_acc: 0.9551 train_loss: 0.118414\tval_acc: 1.000000 val_loss: 0.0067723\n",
      "8_840 train_acc: 0.9663 train_loss: 0.113824\tval_acc: 1.000000 val_loss: 0.0064387\n",
      "8_846 train_acc: 0.9363 train_loss: 0.172417\tval_acc: 1.000000 val_loss: 0.0040466\n",
      "8_941 train_acc: 0.9588 train_loss: 0.140161\tval_acc: 1.000000 val_loss: 0.0037476\n",
      "8_975 train_acc: 0.9551 train_loss: 0.140026\tval_acc: 1.000000 val_loss: 0.0021546\n",
      "epoch:  975 \tThe test accuracy is: 0.8472222222222222\n",
      " THE BEST ACCURACY IS 0.8472222222222222\tkappa is 0.7962962962962963\n",
      "subject 8 duration: 0:47:10.309648\n",
      "seed is 480\n",
      "Subject 9\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "9_0 train_acc: 0.2772 train_loss: 2.493927\tval_acc: 0.333333 val_loss: 1.4036765\n",
      "9_1 train_acc: 0.2659 train_loss: 2.136465\tval_acc: 0.348039 val_loss: 1.3732466\n",
      "9_2 train_acc: 0.3146 train_loss: 1.929791\tval_acc: 0.352941 val_loss: 1.3384581\n",
      "9_3 train_acc: 0.3783 train_loss: 1.639315\tval_acc: 0.421569 val_loss: 1.2720090\n",
      "9_4 train_acc: 0.3783 train_loss: 1.565221\tval_acc: 0.446078 val_loss: 1.1955750\n",
      "9_5 train_acc: 0.4120 train_loss: 1.478940\tval_acc: 0.553922 val_loss: 1.1061796\n",
      "9_6 train_acc: 0.3745 train_loss: 1.602990\tval_acc: 0.583333 val_loss: 1.0658778\n",
      "9_7 train_acc: 0.4532 train_loss: 1.369648\tval_acc: 0.602941 val_loss: 0.9994493\n",
      "9_8 train_acc: 0.4157 train_loss: 1.410068\tval_acc: 0.691176 val_loss: 0.9127461\n",
      "9_9 train_acc: 0.4944 train_loss: 1.205745\tval_acc: 0.700980 val_loss: 0.8462408\n",
      "9_10 train_acc: 0.5056 train_loss: 1.233524\tval_acc: 0.759804 val_loss: 0.7990807\n",
      "9_11 train_acc: 0.5019 train_loss: 1.206527\tval_acc: 0.779412 val_loss: 0.7643043\n",
      "9_12 train_acc: 0.4644 train_loss: 1.210743\tval_acc: 0.754902 val_loss: 0.7615169\n",
      "9_13 train_acc: 0.5468 train_loss: 1.047693\tval_acc: 0.803922 val_loss: 0.6771905\n",
      "9_14 train_acc: 0.6105 train_loss: 0.946799\tval_acc: 0.813725 val_loss: 0.6656326\n",
      "9_15 train_acc: 0.6517 train_loss: 0.890546\tval_acc: 0.808824 val_loss: 0.6320223\n",
      "9_16 train_acc: 0.6479 train_loss: 0.915998\tval_acc: 0.789216 val_loss: 0.6194731\n",
      "9_17 train_acc: 0.5993 train_loss: 0.980367\tval_acc: 0.803922 val_loss: 0.6148462\n",
      "9_18 train_acc: 0.5918 train_loss: 0.916531\tval_acc: 0.808824 val_loss: 0.5579619\n",
      "9_19 train_acc: 0.6629 train_loss: 0.889185\tval_acc: 0.823529 val_loss: 0.5477858\n",
      "9_21 train_acc: 0.6330 train_loss: 0.943012\tval_acc: 0.867647 val_loss: 0.5113881\n",
      "9_23 train_acc: 0.6629 train_loss: 0.816302\tval_acc: 0.862745 val_loss: 0.4910266\n",
      "9_24 train_acc: 0.6517 train_loss: 0.859553\tval_acc: 0.843137 val_loss: 0.4608238\n",
      "9_25 train_acc: 0.6779 train_loss: 0.772722\tval_acc: 0.872549 val_loss: 0.4376374\n",
      "9_27 train_acc: 0.7079 train_loss: 0.741130\tval_acc: 0.862745 val_loss: 0.4277717\n",
      "9_29 train_acc: 0.7378 train_loss: 0.692649\tval_acc: 0.872549 val_loss: 0.4167059\n",
      "9_30 train_acc: 0.7416 train_loss: 0.664874\tval_acc: 0.887255 val_loss: 0.4016500\n",
      "9_31 train_acc: 0.7378 train_loss: 0.730384\tval_acc: 0.901961 val_loss: 0.3394650\n",
      "9_38 train_acc: 0.7378 train_loss: 0.628051\tval_acc: 0.887255 val_loss: 0.3251555\n",
      "9_44 train_acc: 0.7715 train_loss: 0.557543\tval_acc: 0.877451 val_loss: 0.3186272\n",
      "9_49 train_acc: 0.7715 train_loss: 0.551959\tval_acc: 0.901961 val_loss: 0.2848150\n",
      "9_57 train_acc: 0.7940 train_loss: 0.573620\tval_acc: 0.906863 val_loss: 0.2604612\n",
      "9_58 train_acc: 0.8277 train_loss: 0.500343\tval_acc: 0.901961 val_loss: 0.2519749\n",
      "9_63 train_acc: 0.7940 train_loss: 0.555972\tval_acc: 0.941176 val_loss: 0.2116179\n",
      "9_75 train_acc: 0.8352 train_loss: 0.403364\tval_acc: 0.911765 val_loss: 0.1855434\n",
      "9_88 train_acc: 0.8652 train_loss: 0.403773\tval_acc: 0.936275 val_loss: 0.1714621\n",
      "9_94 train_acc: 0.8652 train_loss: 0.367346\tval_acc: 0.931373 val_loss: 0.1621941\n",
      "9_100 train_acc: 0.8989 train_loss: 0.313103\tval_acc: 0.926471 val_loss: 0.1608115\n",
      "9_102 train_acc: 0.8614 train_loss: 0.370523\tval_acc: 0.941176 val_loss: 0.1426322\n",
      "9_106 train_acc: 0.8502 train_loss: 0.376892\tval_acc: 0.946078 val_loss: 0.1162358\n",
      "9_136 train_acc: 0.8876 train_loss: 0.290293\tval_acc: 0.950980 val_loss: 0.1117835\n",
      "9_137 train_acc: 0.8539 train_loss: 0.397969\tval_acc: 0.960784 val_loss: 0.0933926\n",
      "9_148 train_acc: 0.9176 train_loss: 0.233303\tval_acc: 0.980392 val_loss: 0.0794935\n",
      "9_173 train_acc: 0.9139 train_loss: 0.233073\tval_acc: 0.975490 val_loss: 0.0772239\n",
      "9_178 train_acc: 0.8801 train_loss: 0.319393\tval_acc: 0.970588 val_loss: 0.0729403\n",
      "9_189 train_acc: 0.9213 train_loss: 0.218223\tval_acc: 0.980392 val_loss: 0.0700959\n",
      "9_191 train_acc: 0.9251 train_loss: 0.217326\tval_acc: 0.980392 val_loss: 0.0632909\n",
      "9_197 train_acc: 0.9026 train_loss: 0.248154\tval_acc: 0.980392 val_loss: 0.0607050\n",
      "9_212 train_acc: 0.8764 train_loss: 0.323615\tval_acc: 0.990196 val_loss: 0.0596228\n",
      "9_232 train_acc: 0.9288 train_loss: 0.176657\tval_acc: 0.980392 val_loss: 0.0548831\n",
      "9_234 train_acc: 0.9213 train_loss: 0.207928\tval_acc: 0.985294 val_loss: 0.0471990\n",
      "9_242 train_acc: 0.9139 train_loss: 0.211494\tval_acc: 0.995098 val_loss: 0.0442410\n",
      "9_245 train_acc: 0.9176 train_loss: 0.232158\tval_acc: 0.995098 val_loss: 0.0431416\n",
      "9_247 train_acc: 0.9251 train_loss: 0.214059\tval_acc: 0.985294 val_loss: 0.0381075\n",
      "9_278 train_acc: 0.9363 train_loss: 0.192080\tval_acc: 0.995098 val_loss: 0.0324319\n",
      "9_285 train_acc: 0.9363 train_loss: 0.152600\tval_acc: 0.995098 val_loss: 0.0242299\n",
      "9_298 train_acc: 0.9326 train_loss: 0.198212\tval_acc: 0.995098 val_loss: 0.0236399\n",
      "9_326 train_acc: 0.9288 train_loss: 0.186470\tval_acc: 1.000000 val_loss: 0.0131553\n",
      "9_441 train_acc: 0.9513 train_loss: 0.125709\tval_acc: 1.000000 val_loss: 0.0090089\n",
      "9_499 train_acc: 0.9476 train_loss: 0.113065\tval_acc: 1.000000 val_loss: 0.0088804\n",
      "9_506 train_acc: 0.9476 train_loss: 0.130029\tval_acc: 1.000000 val_loss: 0.0087716\n",
      "9_510 train_acc: 0.9588 train_loss: 0.123518\tval_acc: 1.000000 val_loss: 0.0038840\n",
      "9_634 train_acc: 0.9551 train_loss: 0.099509\tval_acc: 1.000000 val_loss: 0.0033795\n",
      "9_638 train_acc: 0.9625 train_loss: 0.076016\tval_acc: 1.000000 val_loss: 0.0033774\n",
      "9_692 train_acc: 0.9663 train_loss: 0.090668\tval_acc: 1.000000 val_loss: 0.0030457\n",
      "9_708 train_acc: 0.9663 train_loss: 0.106185\tval_acc: 1.000000 val_loss: 0.0027878\n",
      "9_723 train_acc: 0.9738 train_loss: 0.105448\tval_acc: 1.000000 val_loss: 0.0024478\n",
      "9_733 train_acc: 0.9625 train_loss: 0.107432\tval_acc: 1.000000 val_loss: 0.0023204\n",
      "9_747 train_acc: 0.9700 train_loss: 0.096769\tval_acc: 1.000000 val_loss: 0.0018639\n",
      "9_774 train_acc: 0.9738 train_loss: 0.076063\tval_acc: 1.000000 val_loss: 0.0017371\n",
      "9_775 train_acc: 0.9513 train_loss: 0.128032\tval_acc: 1.000000 val_loss: 0.0015696\n",
      "9_789 train_acc: 0.9700 train_loss: 0.065016\tval_acc: 1.000000 val_loss: 0.0014145\n",
      "9_902 train_acc: 0.9700 train_loss: 0.095930\tval_acc: 1.000000 val_loss: 0.0013755\n",
      "9_935 train_acc: 0.9700 train_loss: 0.102372\tval_acc: 1.000000 val_loss: 0.0010568\n",
      "9_971 train_acc: 0.9850 train_loss: 0.063180\tval_acc: 1.000000 val_loss: 0.0008198\n",
      "9_979 train_acc: 0.9438 train_loss: 0.124308\tval_acc: 1.000000 val_loss: 0.0006325\n",
      "epoch:  979 \tThe test accuracy is: 0.84375\n",
      " THE BEST ACCURACY IS 0.84375\tkappa is 0.7916666666666666\n",
      "subject 9 duration: 0:47:13.558196\n",
      "**The average Best accuracy is: 81.13425925925925kappa is: 74.84567901234567\n",
      "\n",
      "best epochs:  [977, 993, 985, 990, 994, 982, 941, 975, 979]\n",
      "---------  all result  ---------\n",
      "        accuray  precision     recall         f1      kappa\n",
      "0     90.625000  90.721086  90.625000  90.581024  87.500000\n",
      "1     62.152778  67.598926  62.152778  61.471553  49.537037\n",
      "2     92.013889  92.220653  92.013889  91.987650  89.351852\n",
      "3     84.722222  85.273713  84.722222  84.718836  79.629630\n",
      "4     75.347222  75.336424  75.347222  75.172513  67.129630\n",
      "5     65.625000  65.827206  65.625000  65.660675  54.166667\n",
      "6     90.625000  91.003054  90.625000  90.525374  87.500000\n",
      "7     84.722222  85.333795  84.722222  84.726174  79.629630\n",
      "8     84.375000  85.069831  84.375000  84.200102  79.166667\n",
      "mean  81.134259  82.042743  81.134259  81.004878  74.845679\n",
      "std   11.006178  10.038466  11.006178  11.134555  14.674904\n",
      "****************************************\n",
      "Wed Jul 30 10:55:40 2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import calMetrics\n",
    "from utils import calculatePerClass\n",
    "from utils import numberClassChannel\n",
    "from utils import load_data_evaluate\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Set device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, f1=16, kernel_size=64, D=2, pooling_size1=8, pooling_size2=8, dropout_rate=0.3, number_channel=22, emb_size=40):\n",
    "        super().__init__()\n",
    "        f2 = D*f1\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            nn.Conv2d(1, f1, (1, kernel_size), (1, 1), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.Conv2d(f1, f2, (number_channel, 1), (1, 1), groups=f1, padding='valid', bias=False),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, pooling_size1)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Conv2d(f2, f2, (1, 16), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, pooling_size2)),\n",
    "            nn.Dropout(dropout_rate),  \n",
    "        )\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.cnn_module(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, flatten_number, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(flatten_number, n_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn, emb_size, drop_p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "        self.layernorm = nn.LayerNorm(emb_size)\n",
    "    def forward(self, x, **kwargs):\n",
    "        x_input = x\n",
    "        res = self.fn(x, **kwargs)\n",
    "        out = self.layernorm(self.drop(res)+x_input)\n",
    "        return out\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=4,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                ), emb_size, drop_p),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                ), emb_size, drop_p)\n",
    "            )    \n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, heads, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size, heads) for _ in range(depth)])\n",
    "\n",
    "class BranchEEGNetTransformer(nn.Sequential):\n",
    "    def __init__(self, heads=4, \n",
    "                 depth=6, \n",
    "                 emb_size=40, \n",
    "                 number_channel=22,\n",
    "                 f1 = 20,\n",
    "                 kernel_size = 64,\n",
    "                 D = 2,\n",
    "                 pooling_size1 = 8,\n",
    "                 pooling_size2 = 8,\n",
    "                 dropout_rate = 0.3,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbeddingCNN(f1=f1, \n",
    "                                 kernel_size=kernel_size,\n",
    "                                 D=D, \n",
    "                                 pooling_size1=pooling_size1, \n",
    "                                 pooling_size2=pooling_size2, \n",
    "                                 dropout_rate=dropout_rate,\n",
    "                                 number_channel=number_channel,\n",
    "                                 emb_size=emb_size),\n",
    "        )\n",
    "\n",
    "class PositioinalEncoding(nn.Module):\n",
    "    def __init__(self, embedding, length=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoding = nn.Parameter(torch.randn(1, length, embedding))\n",
    "    def forward(self, x):\n",
    "        x = x + self.encoding[:, :x.shape[1], :].to(x.device)\n",
    "        return self.dropout(x)        \n",
    "\n",
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 database_type='A', \n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 eeg1_number_channel = 22,\n",
    "                 flatten_eeg1 = 600,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "        self.emb_size = emb_size\n",
    "        self.flatten_eeg1 = flatten_eeg1\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.cnn = BranchEEGNetTransformer(heads, depth, emb_size, number_channel=self.number_channel,\n",
    "                                              f1 = eeg1_f1,\n",
    "                                              kernel_size = eeg1_kernel_size,\n",
    "                                              D = eeg1_D,\n",
    "                                              pooling_size1 = eeg1_pooling_size1,\n",
    "                                              pooling_size2 = eeg1_pooling_size2,\n",
    "                                              dropout_rate = eeg1_dropout_rate,\n",
    "                                              )\n",
    "        self.position = PositioinalEncoding(emb_size, dropout=0.1)\n",
    "        self.trans = TransformerEncoder(heads, depth, emb_size)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classification = ClassificationHead(self.flatten_eeg1 , self.number_class)\n",
    "    def forward(self, x):\n",
    "        cnn = self.cnn(x)\n",
    "        cnn = cnn * math.sqrt(self.emb_size)\n",
    "        cnn = self.position(cnn)\n",
    "        trans = self.trans(cnn)\n",
    "        features = cnn+trans\n",
    "        out = self.classification(self.flatten(features))\n",
    "        return features, out\n",
    "\n",
    "class ExP():\n",
    "    def __init__(self, nsub, data_dir, result_name, \n",
    "                 epochs=2000, \n",
    "                 number_aug=2,\n",
    "                 number_seg=8, \n",
    "                 gpus=[0], \n",
    "                 evaluate_mode = 'subject-dependent',\n",
    "                 heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 dataset_type='A',\n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 flatten_eeg1 = 600, \n",
    "                 validate_ratio = 0.2,\n",
    "                 learning_rate = 0.001,\n",
    "                 batch_size = 72,  \n",
    "                 ):\n",
    "        super(ExP, self).__init__()\n",
    "        self.dataset_type = dataset_type\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_epochs = epochs\n",
    "        self.nSub = nsub\n",
    "        self.number_augmentation = number_aug\n",
    "        self.number_seg = number_seg\n",
    "        self.root = data_dir\n",
    "        self.heads=heads\n",
    "        self.emb_size=emb_size\n",
    "        self.depth=depth\n",
    "        self.result_name = result_name\n",
    "        self.evaluate_mode = evaluate_mode\n",
    "        self.validate_ratio = validate_ratio\n",
    "\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "        self.number_class, self.number_channel = numberClassChannel(self.dataset_type)\n",
    "        self.model = EEGTransformer(\n",
    "             heads=self.heads, \n",
    "             emb_size=self.emb_size,\n",
    "             depth=self.depth, \n",
    "            database_type=self.dataset_type, \n",
    "            eeg1_f1=eeg1_f1, \n",
    "            eeg1_D=eeg1_D,\n",
    "            eeg1_kernel_size=eeg1_kernel_size,\n",
    "            eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "            eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "            eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "            eeg1_number_channel = self.number_channel,\n",
    "            flatten_eeg1 = flatten_eeg1,  \n",
    "            ).to(device)\n",
    "        self.model_filename = self.result_name + '/model_{}.pth'.format(self.nSub)\n",
    "\n",
    "    def interaug(self, timg, label):  \n",
    "        aug_data = []\n",
    "        aug_label = []\n",
    "        number_records_by_augmentation = self.number_augmentation * int(self.batch_size / self.number_class)\n",
    "        number_segmentation_points = 1000 // self.number_seg\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            tmp_label = label[cls_idx]\n",
    "            tmp_aug_data = np.zeros((number_records_by_augmentation, 1, self.number_channel, 1000))\n",
    "            for ri in range(number_records_by_augmentation):\n",
    "                for rj in range(self.number_seg):\n",
    "                    rand_idx = np.random.randint(0, tmp_data.shape[0], self.number_seg)\n",
    "                    tmp_aug_data[ri, :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points] = \\\n",
    "                        tmp_data[rand_idx[rj], :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points]\n",
    "            aug_data.append(tmp_aug_data)\n",
    "            aug_label.append(tmp_label[:number_records_by_augmentation])\n",
    "        aug_data = np.concatenate(aug_data)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        aug_shuffle = np.random.permutation(len(aug_data))\n",
    "        aug_data = aug_data[aug_shuffle, :, :]\n",
    "        aug_label = aug_label[aug_shuffle]\n",
    "        aug_data = torch.from_numpy(aug_data).to(device).float()\n",
    "        aug_label = torch.from_numpy(aug_label-1).to(device).long()\n",
    "        return aug_data, aug_label\n",
    "\n",
    "    def get_source_data(self):\n",
    "        (self.train_data,\n",
    "         self.train_label, \n",
    "         self.test_data, \n",
    "         self.test_label) = load_data_evaluate(self.root, self.dataset_type, self.nSub, mode_evaluate=self.evaluate_mode)\n",
    "        self.train_data = np.expand_dims(self.train_data, axis=1)\n",
    "        self.train_label = np.transpose(self.train_label)\n",
    "        self.allData = self.train_data\n",
    "        self.allLabel = self.train_label[0]\n",
    "        shuffle_num = np.random.permutation(len(self.allData))\n",
    "        self.allData = self.allData[shuffle_num, :, :, :]\n",
    "        self.allLabel = self.allLabel[shuffle_num]\n",
    "        print('-'*20, \"train size：\", self.train_data.shape, \"test size：\", self.test_data.shape)\n",
    "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
    "        self.test_label = np.transpose(self.test_label)\n",
    "        self.testData = self.test_data\n",
    "        self.testLabel = self.test_label[0]\n",
    "        target_mean = np.mean(self.allData)\n",
    "        target_std = np.std(self.allData)\n",
    "        self.allData = (self.allData - target_mean) / target_std\n",
    "        self.testData = (self.testData - target_mean) / target_std\n",
    "        isSaveDataLabel = False\n",
    "        if isSaveDataLabel:\n",
    "            np.save(\"./gradm_data/train_data_{}.npy\".format(self.nSub), self.allData)\n",
    "            np.save(\"./gradm_data/train_lable_{}.npy\".format(self.nSub), self.allLabel)\n",
    "            np.save(\"./gradm_data/test_data_{}.npy\".format(self.nSub), self.testData)\n",
    "            np.save(\"./gradm_data/test_label_{}.npy\".format(self.nSub), self.testLabel)\n",
    "        return self.allData, self.allLabel, self.testData, self.testLabel\n",
    "\n",
    "    def train(self):\n",
    "        img, label, test_data, test_label = self.get_source_data()\n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(label - 1)\n",
    "        dataset = torch.utils.data.TensorDataset(img, label)\n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_label = torch.from_numpy(test_label - 1)\n",
    "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "        test_data = test_data.to(device).float()\n",
    "        test_label = test_label.to(device).long()\n",
    "        best_epoch = 0\n",
    "        num = 0\n",
    "        min_loss = 100\n",
    "        result_process = []\n",
    "        for e in range(self.n_epochs):\n",
    "            self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_process = {}\n",
    "            epoch_process['epoch'] = e\n",
    "            self.model.train()\n",
    "            outputs_list = []\n",
    "            label_list = []\n",
    "            val_data_list = []\n",
    "            val_label_list = []\n",
    "            for i, (img, label) in enumerate(self.dataloader):\n",
    "                number_sample = img.shape[0]\n",
    "                number_validate = int(self.validate_ratio * number_sample)\n",
    "                train_data = img[:-number_validate]\n",
    "                train_label = label[:-number_validate]\n",
    "                val_data_list.append(img[number_validate:])\n",
    "                val_label_list.append(label[number_validate:])\n",
    "                img = train_data.to(device).float()\n",
    "                label = train_label.to(device).long()\n",
    "                aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "                img = torch.cat((img, aug_data))\n",
    "                label = torch.cat((label, aug_label))\n",
    "                features, outputs = self.model(img)\n",
    "                outputs_list.append(outputs)\n",
    "                label_list.append(label)\n",
    "                loss = self.criterion_cls(outputs, label) \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            del img\n",
    "            torch.cuda.empty_cache()\n",
    "            if (e + 1) % 1 == 0:\n",
    "                self.model.eval()\n",
    "                val_data = torch.cat(val_data_list).to(device).float()\n",
    "                val_label = torch.cat(val_label_list).to(device).long()\n",
    "                val_dataset = torch.utils.data.TensorDataset(val_data, val_label)\n",
    "                self.val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "                outputs_list = []\n",
    "                with torch.no_grad():\n",
    "                    for i, (img, _) in enumerate(self.val_dataloader):\n",
    "                        img = img.to(device).float()\n",
    "                        _, Cls = self.model(img)\n",
    "                        outputs_list.append(Cls)\n",
    "                        del img, Cls\n",
    "                        torch.cuda.empty_cache()\n",
    "                Cls = torch.cat(outputs_list)\n",
    "                val_loss = self.criterion_cls(Cls, val_label)\n",
    "                val_pred = torch.max(Cls, 1)[1]\n",
    "                val_acc = float((val_pred == val_label).cpu().numpy().astype(int).sum()) / float(val_label.size(0))\n",
    "                epoch_process['val_acc'] = val_acc                \n",
    "                epoch_process['val_loss'] = val_loss.detach().cpu().numpy()  \n",
    "                train_pred = torch.max(outputs, 1)[1]\n",
    "                train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
    "                epoch_process['train_acc'] = train_acc\n",
    "                epoch_process['train_loss'] = loss.detach().cpu().numpy()\n",
    "                num = num + 1\n",
    "                if min_loss>val_loss:\n",
    "                    min_loss = val_loss\n",
    "                    best_epoch = e\n",
    "                    epoch_process['epoch'] = e\n",
    "                    torch.save(self.model, self.model_filename)\n",
    "                    print(\"{}_{} train_acc: {:.4f} train_loss: {:.6f}\\tval_acc: {:.6f} val_loss: {:.7f}\".format(self.nSub,\n",
    "                                                                                           epoch_process['epoch'],\n",
    "                                                                                           epoch_process['train_acc'],\n",
    "                                                                                           epoch_process['train_loss'],\n",
    "                                                                                           epoch_process['val_acc'],\n",
    "                                                                                           epoch_process['val_loss'],\n",
    "                                                                                        ))\n",
    "            result_process.append(epoch_process)  \n",
    "            del label, val_data, val_label\n",
    "            torch.cuda.empty_cache()\n",
    "        self.model.eval()\n",
    "        self.model = torch.load(self.model_filename, map_location=device)\n",
    "        outputs_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in enumerate(self.test_dataloader):\n",
    "                img_test = img.to(device).float()\n",
    "                features, outputs = self.model(img_test)\n",
    "                val_pred = torch.max(outputs, 1)[1]\n",
    "                outputs_list.append(outputs)\n",
    "        outputs = torch.cat(outputs_list) \n",
    "        y_pred = torch.max(outputs, 1)[1]\n",
    "        test_acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
    "        print(\"epoch: \", best_epoch, '\\tThe test accuracy is:', test_acc)\n",
    "        df_process = pd.DataFrame(result_process)\n",
    "        return test_acc, test_label, y_pred, df_process, best_epoch\n",
    "\n",
    "def main(dirs,                \n",
    "         evaluate_mode = 'subject-dependent',\n",
    "         heads=8,\n",
    "         emb_size=48,\n",
    "         depth=3,\n",
    "         dataset_type='A',\n",
    "         eeg1_f1=20,\n",
    "         eeg1_kernel_size=64,\n",
    "         eeg1_D=2,\n",
    "         eeg1_pooling_size1=8,\n",
    "         eeg1_pooling_size2=8,\n",
    "         eeg1_dropout_rate=0.3,\n",
    "         flatten_eeg1=600,   \n",
    "         validate_ratio = 0.2\n",
    "         ):\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "    result_write_metric = ExcelWriter(dirs+\"/result_metric.xlsx\")\n",
    "    result_metric_dict = {}\n",
    "    y_true_pred_dict = { }\n",
    "    process_write = ExcelWriter(dirs+\"/process_train.xlsx\")\n",
    "    pred_true_write = ExcelWriter(dirs+\"/pred_true.xlsx\")\n",
    "    subjects_result = []\n",
    "    best_epochs = []\n",
    "    for i in range(N_SUBJECT):      \n",
    "        starttime = datetime.datetime.now()\n",
    "        seed_n = np.random.randint(2024)\n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        index_round =0\n",
    "        print('Subject %d' % (i+1))\n",
    "        exp = ExP(i + 1, DATA_DIR, dirs, EPOCHS, N_AUG, N_SEG, None, \n",
    "                  evaluate_mode = evaluate_mode,\n",
    "                  heads=heads, \n",
    "                  emb_size=emb_size,\n",
    "                  depth=depth, \n",
    "                  dataset_type=dataset_type,\n",
    "                  eeg1_f1 = eeg1_f1,\n",
    "                  eeg1_kernel_size = eeg1_kernel_size,\n",
    "                  eeg1_D = eeg1_D,\n",
    "                  eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "                  eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "                  eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "                  flatten_eeg1 = flatten_eeg1,  \n",
    "                  validate_ratio = validate_ratio\n",
    "                  )\n",
    "        testAcc, Y_true, Y_pred, df_process, best_epoch = exp.train()\n",
    "        true_cpu = Y_true.cpu().numpy().astype(int)\n",
    "        pred_cpu = Y_pred.cpu().numpy().astype(int)\n",
    "        df_pred_true = pd.DataFrame({'pred': pred_cpu, 'true': true_cpu})\n",
    "        df_pred_true.to_excel(pred_true_write, sheet_name=str(i+1))\n",
    "        y_true_pred_dict[i] = df_pred_true\n",
    "        accuracy, precison, recall, f1, kappa = calMetrics(true_cpu, pred_cpu)\n",
    "        subject_result = {'accuray': accuracy*100,\n",
    "                          'precision': precison*100,\n",
    "                          'recall': recall*100,\n",
    "                          'f1': f1*100, \n",
    "                          'kappa': kappa*100\n",
    "                          }\n",
    "        subjects_result.append(subject_result)\n",
    "        df_process.to_excel(process_write, sheet_name=str(i+1))\n",
    "        best_epochs.append(best_epoch)\n",
    "        print(' THE BEST ACCURACY IS ' + str(testAcc) + \"\\tkappa is \" + str(kappa) )\n",
    "        endtime = datetime.datetime.now()\n",
    "        print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "        if i == 0:\n",
    "            yt = Y_true\n",
    "            yp = Y_pred\n",
    "        else:\n",
    "            yt = torch.cat((yt, Y_true))\n",
    "            yp = torch.cat((yp, Y_pred))\n",
    "        df_result = pd.DataFrame(subjects_result)\n",
    "    process_write.close()\n",
    "    pred_true_write.close()\n",
    "    print('**The average Best accuracy is: ' + str(df_result['accuray'].mean()) + \"kappa is: \" + str(df_result['kappa'].mean()) + \"\\n\" )\n",
    "    print(\"best epochs: \", best_epochs)\n",
    "    result_metric_dict = df_result\n",
    "    mean = df_result.mean(axis=0)\n",
    "    mean.name = 'mean'\n",
    "    std = df_result.std(axis=0)\n",
    "    std.name = 'std'\n",
    "    df_result = pd.concat([df_result, pd.DataFrame(mean).T, pd.DataFrame(std).T])\n",
    "    df_result.to_excel(result_write_metric, index=False)\n",
    "    print('-'*9, ' all result ', '-'*9)\n",
    "    print(df_result)\n",
    "    print(\"*\"*40)\n",
    "    result_write_metric.close()\n",
    "    return result_metric_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_DIR = r'./mymat_raw/'\n",
    "    EVALUATE_MODE = 'LOSO-No'\n",
    "    N_SUBJECT = 9\n",
    "    N_AUG = 3\n",
    "    N_SEG = 8\n",
    "    EPOCHS = 1000\n",
    "    EMB_DIM = 16\n",
    "    HEADS = 2\n",
    "    DEPTH = 6\n",
    "    TYPE = 'B'\n",
    "    validate_ratio = 0.3\n",
    "    EEGNet1_F1 = 8\n",
    "    EEGNet1_KERNEL_SIZE=64\n",
    "    EEGNet1_D=2\n",
    "    EEGNet1_POOL_SIZE1 = 8\n",
    "    EEGNet1_POOL_SIZE2 = 8\n",
    "    FLATTEN_EEGNet1 = 240\n",
    "    if EVALUATE_MODE!='LOSO':\n",
    "        EEGNet1_DROPOUT_RATE = 0.5\n",
    "    else:\n",
    "        EEGNet1_DROPOUT_RATE = 0.25    \n",
    "    parameters_list = ['A']\n",
    "    for TYPE in parameters_list:\n",
    "        number_class, number_channel = numberClassChannel(TYPE)\n",
    "        RESULT_NAME = \"CTNet_{}_heads_{}_depth_{}_{}\".format(TYPE, HEADS, DEPTH, int(time.time()))\n",
    "        sModel = EEGTransformer(\n",
    "            heads=HEADS, \n",
    "            emb_size=EMB_DIM,\n",
    "            depth=DEPTH, \n",
    "            database_type=TYPE,\n",
    "            eeg1_f1=EEGNet1_F1, \n",
    "            eeg1_D=EEGNet1_D,\n",
    "            eeg1_kernel_size=EEGNet1_KERNEL_SIZE,\n",
    "            eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "            eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "            eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "            eeg1_number_channel = number_channel,\n",
    "            flatten_eeg1 = FLATTEN_EEGNet1,  \n",
    "            ).to(device)\n",
    "        summary(sModel, (1, number_channel, 1000)) \n",
    "        print(time.asctime(time.localtime(time.time())))\n",
    "        result = main(RESULT_NAME,\n",
    "                        evaluate_mode = EVALUATE_MODE,\n",
    "                        heads=HEADS, \n",
    "                        emb_size=EMB_DIM,\n",
    "                        depth=DEPTH, \n",
    "                        dataset_type=TYPE,\n",
    "                        eeg1_f1 = EEGNet1_F1,\n",
    "                        eeg1_kernel_size = EEGNet1_KERNEL_SIZE,\n",
    "                        eeg1_D = EEGNet1_D,\n",
    "                        eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "                        eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "                        eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "                        flatten_eeg1 = FLATTEN_EEGNet1,\n",
    "                        validate_ratio = validate_ratio,\n",
    "                      )\n",
    "        print(time.asctime(time.localtime(time.time())))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
